{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating FGDC Triples\n",
    "===================\n",
    "\n",
    "\n",
    "This notebook does 2 things, first it retrieves a Solr query with FGDC documents and uses the current parsers to create a directory with TTL files. Second, upload these TTL files to Parliament.\n",
    "Adjust the variables to specific needs.\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "Variables\n",
    "-------------\n",
    "\n",
    "* **solr_endpoint**:  The Solr URL.\n",
    "* **solr_query**:  Solr query, note that for this notebook this is hard coded to just FGDC docs.\n",
    "* **solr_num_docs**: Number of documents to triplelize\n",
    "* **solr_user**: Solr username\n",
    "* **solr_password**: Solr password\n",
    "* **dump_dir**: Where the ttl files are going\n",
    "* **parliament_endpont**: Parliament **Sparql** endpoint\n",
    "* **parliament_user**:  For future use\n",
    "* **parliament_password**: For future use\n",
    "* **parliament_graph**: The graph we are going to write to... now this should be dev, test but probably never PROD.\n",
    "\n",
    "\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from rdflib import Graph, URIRef\n",
    "from rdflib.plugins.stores import sparqlstore\n",
    "\n",
    "# If we execute this notebook in the top level we don't need this line\n",
    "sys.path.append('../../')\n",
    "\n",
    "# BCube imports\n",
    "from semproc.preprocessors.metadata_preprocessors import FgdcItemReader\n",
    "from semproc.serializers.rdfgraphs import RdfGrapher\n",
    "from semproc.parser import Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables, some are set on the server side for security.\n",
    "\n",
    "# Solr\n",
    "solr_endpoint = os.environ['SOLR_URL']\n",
    "solr_query = 'q=raw_content%3A\"*FGDC-STD-001-1998*\"&sort=date+desc&fl=id%2Cdate%2Craw_content%2Curl_hash&wt=json&indent=true'\n",
    "solr_num_docs = 50\n",
    "solr_page_size = 10\n",
    "solr_user = os.environ['SOLR_USER']\n",
    "solr_password = os.environ['SOLR_PASS']\n",
    "dump_dir = '../../fgdc_triples'\n",
    "\n",
    "# Parliament\n",
    "parliament_endpoint = os.environ['PARLIAMENT_ENDPOINT']\n",
    "parliament_graph = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 7 files where triplelized and 3 couldn't be parsed.\n",
      "A total of 2 files where triplelized and 8 couldn't be parsed.\n",
      "A total of 2 files where triplelized and 8 couldn't be parsed.\n",
      "A total of 5 files where triplelized and 5 couldn't be parsed.\n",
      "A total of 4 files where triplelized and 6 couldn't be parsed.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve Solr documents, parse them, triplelize them and store the ttl's in dump_dir\n",
    "\n",
    "# First purge the output dir\n",
    "if os.path.exists(dump_dir):\n",
    "    shutil.rmtree(dump_dir)\n",
    "os.makedirs(dump_dir)\n",
    "\n",
    "# We need to remove special chars that might be present\n",
    "def sanitize_content(content):\n",
    "    sanitized_content = content.replace('\\\\\\n', '').replace('\\r\\n', '').\\\n",
    "    replace('\\\\r', '').replace('\\\\n', '').replace('\\n', '')\n",
    "    return sanitized_content.encode('unicode_escape')\n",
    "\n",
    "# Return the ttl representation of parsed_json\n",
    "def triplelize(parsed_json):\n",
    "    triplelizer = RdfGrapher(parsed_json)\n",
    "    triplelizer.serialize()\n",
    "    return triplelizer.emit_format()\n",
    "\n",
    "# Get the documents in a paginated way and create the turtle file\n",
    "for num_docs in range(0, solr_num_docs, solr_page_size):\n",
    "    # for this round we init the counters\n",
    "    triplelized = 0\n",
    "    parse_errors = 0\n",
    "    s_query = solr_endpoint + '/collection1/select?' + solr_query + \\\n",
    "    '&rows=' + str(solr_page_size) + '&start=' + str(num_docs)\n",
    "    # print ('Solr query: {0}'.format(s_query))    \n",
    "    # Retrieve documents\n",
    "    r = requests.get(s_query, auth=HTTPBasicAuth(solr_user,solr_password))\n",
    "    # Load the documents using UTF-8\n",
    "    data = json.loads(r.content.decode(encoding='UTF-8'))\n",
    "    # Note that we are generating one ttl for each valid parsed document\n",
    "    # we could optimize this by hainvg one ttl per page.\n",
    "    for doc in data['response']['docs']:\n",
    "        content = sanitize_content(doc['raw_content'])\n",
    "        parser = Parser(content)\n",
    "        reader = FgdcItemReader(parser.xml, doc['id'], doc['date'])\n",
    "        try:\n",
    "            parsed_json = reader.parse_item()\n",
    "            triplelized += 1\n",
    "        except AttributeError:\n",
    "            parsed_json = None            \n",
    "        if parsed_json is not None:\n",
    "            triples = triplelize(parsed_json)\n",
    "            file_name = dump_dir + '/' + doc['url_hash'] + '.ttl'\n",
    "            with open(file_name, \"w\") as ttl_file:\n",
    "               ttl_file.write(triples)\n",
    "            # uncomment the next line just for dev purposes.\n",
    "            # print ('Triplelized ' + doc['id'] + ' as ' + file_name + '\\n')\n",
    "        else:\n",
    "            parse_errors += 1\n",
    "    print ('A total of {0} files where triplelized and {1} couldn\\'t be parsed.'.format(triplelized, parse_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending: 20 ttl files to Parliament\n",
      "Successfully inserted 20 files in Parliament, 0 failed\n"
     ]
    }
   ],
   "source": [
    "# Finally we are going to send the .ttl files to Parliament in the defined Graph\n",
    "ttls = glob.glob(dump_dir + '/' + '*.ttl')\n",
    "success = 0\n",
    "failed = 0\n",
    "\n",
    "store = sparqlstore.SPARQLUpdateStore(parliament_endpoint, parliament_endpoint)\n",
    "# This should be updated to take advantage of Parliament's bulk update endpoint\n",
    "named_graph = 'urn:' + parliament_graph\n",
    "sg = Graph(store, identifier=URIRef(named_graph))\n",
    "print ('Sending: {0} ttl files to Parliament'.format(len(ttls)))\n",
    "for ttl in ttls:\n",
    "    g = Graph()\n",
    "    g.parse(ttl, format=\"turtle\")\n",
    "    as_nt = g.serialize(format='nt')    \n",
    "    try:\n",
    "        sg.update(\"INSERT DATA { GRAPH <%s> { %s } }\" % (named_graph, as_nt))\n",
    "        success += 1\n",
    "    except:\n",
    "        failed += 1\n",
    "print('Successfully inserted {0} files in Parliament, {1} failed'.format(success, failed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "-----------\n",
    "\n",
    "This notebook uses the python osg library a.k.a. GDAL with all the issues that come from dealing with dependencies.\n",
    "To properly replicate the python environment using a virtualenv is strongly advised. Having a virtual env is just the first step. We must manually install GDAL first, in debian/ubuntu we'll need to:\n",
    "\n",
    "\n",
    "```sh\n",
    "sudo apt-get install build-essential python-all-dev libgdal-dev\n",
    "wget http://download.osgeo.org/gdal/1.11.0/gdal-1.11.0.tar.gz\n",
    "tar xvfz gdal-1.11.0.tar.gz && cd gdal-1.11.0\n",
    "\n",
    "export LD_PRELOAD=/usr/local/lib/libgdal.so.1\n",
    "./configure --with-python\n",
    "make\n",
    "sudo make install\n",
    "```\n",
    "\n",
    "After we have GDAL in our system we can proceed to install the python wrappers:\n",
    "\n",
    "```sh\n",
    "pip install gdal\n",
    "```\n",
    "\n",
    "We can also install using the ones included in GDAL itself:\n",
    "\n",
    "```sh\n",
    "cd /PATH/TO/gdal-1.11.0/swig/python\n",
    "make\n",
    "python setup.py install --prefix=$VIRTUALENV_PATH\n",
    "```\n",
    "\n",
    "* [Set up gdal on Ubuntu 14.04](https://milkator.wordpress.com/2014/05/06/set-up-gdal-on-ubuntu-14-04/)\n",
    "* [Python GDAL package missing header file when installing via pip](https://gis.stackexchange.com/questions/28966/python-gdal-package-missing-header-file-when-installing-via-pip)\n",
    "* [Python gdal undefined symbol GDALRasterBandGetVirtualMem](https://stackoverflow.com/questions/27116402/python-gdal-undefined-symbol-gdalrasterbandgetvirtualmem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
