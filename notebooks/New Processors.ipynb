{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from lxml import etree\n",
    "from HTMLParser import HTMLParser\n",
    "import urlparse\n",
    "import urllib\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### UTILS\n",
    "\n",
    "def generate_localname_xpath(tags):\n",
    "    unchangeds = ['*', '..', '.', '//*']\n",
    "    return '/'.join(\n",
    "        ['%s*[local-name()=\"%s\"]' % ('@' if '@' in t else '', t.replace('@', ''))\n",
    "         if t not in unchangeds else t for t in tags])\n",
    "\n",
    "\n",
    "def extract_attrib(elem, tags):\n",
    "    e = extract_elem(elem, tags)\n",
    "    return e.strip() if e else ''\n",
    "\n",
    "\n",
    "def extract_attribs(elem, tags):\n",
    "    e = extract_elem(elem, tags)\n",
    "    return [m.strip() for m in e]\n",
    "\n",
    "\n",
    "def extract_item(elem, tags):\n",
    "    e = extract_elem(elem, tags)\n",
    "    return e.text.strip() if e is not None and e.text else ''\n",
    "\n",
    "\n",
    "def extract_items(elem, tags):\n",
    "    es = extract_elems(elem, tags)\n",
    "    return [e.text.strip() for e in es if e is not None and e.text]\n",
    "\n",
    "\n",
    "def extract_elems(elem, tags):\n",
    "    xp = generate_localname_xpath(tags)\n",
    "    return elem.xpath(xp)\n",
    "\n",
    "\n",
    "def extract_elem(elem, tags):\n",
    "    xp = generate_localname_xpath(tags)\n",
    "    return next(iter(elem.xpath(xp)), None)\n",
    "\n",
    "\n",
    "def unquote(url):\n",
    "    return urllib.unquote(url)\n",
    "\n",
    "def parse_url(url):\n",
    "    '''\n",
    "    strip out the query parameters\n",
    "    '''\n",
    "    if not url:\n",
    "        return ''\n",
    "    parsed_url = urlparse.urlparse(url)\n",
    "    return urlparse.parse_qs(parsed_url.query)\n",
    "\n",
    "def tidy_dict(items):\n",
    "    # cleanup a dict (remove empty elements)\n",
    "    # but only at the single depth\n",
    "    to_remove = []\n",
    "    for k, v in items.iteritems():\n",
    "        if not v:\n",
    "            to_remove.append(k)\n",
    "    for k in to_remove:\n",
    "        del items[k]\n",
    "\n",
    "    return items\n",
    "\n",
    "def remap_http_method(original_method):\n",
    "        '''\n",
    "        return the \"full\" http method from some input\n",
    "        '''\n",
    "        definition = {\n",
    "            \"HTTP GET\": ['get'],\n",
    "            \"HTTP POST\": ['post']\n",
    "        }\n",
    "        for k, v in definition.iteritems():\n",
    "            if original_method.lower() in v:\n",
    "                return k\n",
    "        return original_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CLASSES\n",
    "\n",
    "class BasicParser():\n",
    "    '''\n",
    "    not concerned about namespaces or querying\n",
    "\n",
    "    note: these could merge at some point\n",
    "    '''\n",
    "    def __init__(self, text):\n",
    "        try:\n",
    "            self.text = text.encode('unicode_escape')\n",
    "        except UnicodeDecodeError:\n",
    "            # TODO: this should be somewhere else and also maybe not this\n",
    "            self.text = text.decode('utf-8', 'replace').encode('unicode_escape')\n",
    "        self.parser = etree.XMLParser(\n",
    "            remove_blank_text=True,\n",
    "            remove_comments=True,\n",
    "            recover=True,\n",
    "            remove_pis=True,\n",
    "            ns_clean=True\n",
    "        )\n",
    "        self._parse()\n",
    "        self._extract_namespaces()\n",
    "\n",
    "    def _parse(self):\n",
    "        try:\n",
    "            self.xml = etree.fromstring(self.text, parser=self.parser)\n",
    "        except Exception as ex:\n",
    "            print ex\n",
    "            raise ex\n",
    "\n",
    "    def _extract_namespaces(self):\n",
    "        '''\n",
    "        Pull all of the namespaces in the source document\n",
    "        and generate a list of tuples (prefix, URI) to dict\n",
    "        '''\n",
    "        if self.xml is None:\n",
    "            self.namespaces = {}\n",
    "            return\n",
    "\n",
    "        document_namespaces = dict(self.xml.xpath('/*/namespace::*'))\n",
    "        if None in document_namespaces:\n",
    "            document_namespaces['default'] = document_namespaces[None]\n",
    "            del document_namespaces[None]\n",
    "\n",
    "        # now run through any child namespace issues\n",
    "        all_namespaces = self.xml.xpath('//namespace::*')\n",
    "        for i, ns in enumerate(all_namespaces):\n",
    "            if ns[1] in document_namespaces.values():\n",
    "                continue\n",
    "            new_key = ns[0] if ns[0] else 'default%s' % i\n",
    "            document_namespaces[new_key] = ns[1]\n",
    "\n",
    "        self.namespaces = document_namespaces\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### the classes we care about from a routing perspective\n",
    "\n",
    "class Processor(object):\n",
    "    '''\n",
    "    where routes is the tag sets to run as namespace-free\n",
    "    xpath. the service, metadata and dataset keys are the dicts of\n",
    "    tag lists (in case we have different locations for y) and the\n",
    "    resultset list is the tag list to the result children\n",
    "    '''\n",
    "\n",
    "    def __init__(self, identify, response, url, parent_url=''):\n",
    "        self.response = response\n",
    "        self.url = url\n",
    "        self.identify = identify\n",
    "        self.parent_url = parent_url\n",
    "\n",
    "        self._load_xml()\n",
    "\n",
    "    def parse(self):\n",
    "        pass\n",
    "    \n",
    "    def parse_children(self, elem=None, tags=[]):\n",
    "        '''\n",
    "        where elem = the parent node for the set and\n",
    "        tags is the un-namespaced list of explicit items\n",
    "        to parse or, if not specified, run the children\n",
    "        one level down\n",
    "        '''\n",
    "        elem = self.parser.xml if elem is None else elem\n",
    "        children = []\n",
    "        if tags:\n",
    "            children = extract_elems(elem, tags)\n",
    "        else:\n",
    "            children = [child for child in elem.iterchildren()]\n",
    "\n",
    "        for child in children:\n",
    "            parsed = self._parse_child(child)\n",
    "            if parsed:\n",
    "                yield parsed\n",
    "        \n",
    "\n",
    "    def _load_xml(self):\n",
    "        self.parser = BasicParser(self.response)\n",
    "    \n",
    "    def _parse_child(self, child):\n",
    "        pass\n",
    "\n",
    "class OpenSearchReader(Processor):\n",
    "    def __init__(self, identify, response, url, parent_url=''):\n",
    "        self.response = response\n",
    "        self.url = url\n",
    "        self.identify = identify\n",
    "        self.parent_url = parent_url\n",
    "        \n",
    "        self._load_xml()\n",
    "        \n",
    "    def parse(self):\n",
    "        self.description = {}\n",
    "        \n",
    "        if self.parent_url:\n",
    "            # TODO: consider making this a sha\n",
    "            self.description['childOf'] = self.parent_url\n",
    "        \n",
    "        if 'service' in self.identify:\n",
    "            self.description['service'] = self._parse_service()\n",
    "            \n",
    "        if 'resultset' in self.identify:\n",
    "            # TODO: get the root stats\n",
    "            self.description['children'] = self._parse_children(self.identify['resultset'].get('dialect', ''))\n",
    "        \n",
    "        self.description = tidy_dict(self.description)\n",
    "\n",
    "    def _parse_service(self):\n",
    "        output = {}\n",
    "        output['title'] = extract_items(self.parser.xml, [\"ShortName\"])        \n",
    "        output['abstract'] = extract_items(self.parser.xml, [\"LongName\"]) + \\\n",
    "            extract_items(self.parser.xml, [\"Description\"])\n",
    "        output['source'] = extract_items(self.parser.xml, [\"Attribution\"])\n",
    "        output['contact'] = extract_items(self.parser.xml, [\"Developer\"])\n",
    "        output['rights'] = extract_items(self.parser.xml, [\"SyndicationRight\"])\n",
    "        output['subject'] = extract_items(self.parser.xml, [\"Tags\"])\n",
    "        \n",
    "        output['endpoints'] = [self._parse_endpoint(e) for e in extract_elems(self.parser.xml, ['Url'])]\n",
    "        \n",
    "        return tidy_dict(output)\n",
    "        \n",
    "    def _parse_endpoint(self, elem):\n",
    "        endpoint = {}\n",
    "        endpoint['protocol'] = remap_http_method(elem.get('type', ''))\n",
    "        endpoint['template'] = elem.get('template', '')\n",
    "        endpoint['parameters'] = self._extract_params(elem)\n",
    "        endpoint['actionable'] = 'NOPE'\n",
    "        endpoint['url'] = ''\n",
    "        \n",
    "        return tidy_dict(endpoint)\n",
    "        \n",
    "    def _parse_children(self, dialect):\n",
    "        ''' i fundamentally do not like this '''\n",
    "        output = {}\n",
    "        \n",
    "        if dialect == 'ATOM':\n",
    "            reader = OpenSearchAtomReader(None, self.response, self.url)\n",
    "        elif dialect == 'RSS':\n",
    "            reader = OpenSearchRssReader(None, self.response, self.url)\n",
    "        return reader.parse()\n",
    "    \n",
    "    def _extract_params(self, endpoint):\n",
    "        def _extract_prefix(param):\n",
    "            pattern = '\\{{0,1}(\\S*):([\\S][^}]*)'\n",
    "\n",
    "            # TODO: this is probably a bad assumption (that there's just the\n",
    "            #   one item in the list, not that urlparse returns the terms as a list)\n",
    "            if isinstance(param, list):\n",
    "                param = param[0]\n",
    "\n",
    "            if ':' not in param:\n",
    "                return ('', param)\n",
    "\n",
    "            m = re.search(pattern, param)\n",
    "            return m.groups()\n",
    "        \n",
    "        _parameter_formats = {\n",
    "            \"geo:box\": \"west, south, east, north\",\n",
    "            \"time:start\": \"YYYY-MM-DDTHH:mm:ssZ\",\n",
    "            \"time:stop\": \"YYYY-MM-DDTHH:mm:ssZ\"\n",
    "        }\n",
    "        url = endpoint.get('template', '')\n",
    "        query_params = parse_url(url)\n",
    "        \n",
    "        # deal with the namespaced parameters as [query param key, prefix, type]\n",
    "        query_params = [[k] + list(_extract_prefix(v)) for k, v\n",
    "                        in query_params.iteritems()]\n",
    "\n",
    "        return [\n",
    "            tidy_dict({\n",
    "                \"name\": qp[0],\n",
    "                \"prefix\": qp[1],\n",
    "                \"type\": qp[2],\n",
    "                \"format\": _parameter_formats.get(':'.join(qp[1:]))\n",
    "                })\n",
    "                for qp in query_params\n",
    "            ]\n",
    "    \n",
    "    \n",
    "class OpenSearchAtomReader(Processor):\n",
    "    def parse(self):\n",
    "        output = {}\n",
    "        output['items'] = [child for child in self.parse_children(tags=['//*', 'entry'])]\n",
    "        \n",
    "        print output\n",
    "        return tidy_dict(output)\n",
    "\n",
    "    def _parse_child(self, child):\n",
    "        entry = {}\n",
    "\n",
    "        entry['title'] = extract_item(child, ['title'])\n",
    "        entry['id'] = extract_item(child, ['id'])\n",
    "        entry['creator'] = extract_item(child, ['creator'])\n",
    "        entry['author'] = extract_item(child, ['author', 'name'])\n",
    "        entry['date'] = extract_item(child, ['date'])\n",
    "        entry['updated'] = extract_item(child, ['updated'])\n",
    "        entry['published'] = extract_item(child, ['published'])\n",
    "\n",
    "        entry['subjects'] = [e.attrib.get('term', '') for e in extract_elems(child, ['category'])]\n",
    "\n",
    "        entry['contents'] = []\n",
    "        contents = extract_elems(child, ['content'])\n",
    "        for content in contents:\n",
    "            text = content.text.strip() if content.text else ''\n",
    "            content_type = content.attrib.get('type', '')\n",
    "            entry['contents'].append({'content': text, 'type': content_type})\n",
    "\n",
    "        entry['links'] = []\n",
    "        links = extract_elems(child, ['link'])\n",
    "        for link in links:\n",
    "            href = link.attrib.get('href', '')\n",
    "            rel = link.attrib.get('rel', '')\n",
    "            entry['links'].append({'href': href, 'rel': rel})\n",
    "\n",
    "        return tidy_dict(entry)\n",
    "\n",
    "class OpenSearchRssReader(Processor):\n",
    "    def parse(self):\n",
    "        output = {}\n",
    "        output['items'] = [child for child in self.parse_children(tags=['//*', 'item'])]\n",
    "        return tidy_dict(output)\n",
    "\n",
    "    def _parse_child(self, child):\n",
    "        item = {}\n",
    "        item['title'] = extract_item(child, ['title'])\n",
    "        item['language'] = extract_item(child, ['language'])\n",
    "        item['author'] = extract_item(child, ['author'])\n",
    "        # TODO: go sort out what this is: http://purl.org/rss/1.0/modules/content/\n",
    "        item['encoded'] = extract_item(child, ['encoded'])\n",
    "        item['id'] = extract_item(child, ['guid'])\n",
    "        item['creator'] = extract_item(child, ['creator'])\n",
    "\n",
    "        item['subjects'] = extract_items(child, ['category'])\n",
    "        item['published'] = extract_item(child, ['pubDate'])\n",
    "        item['timestamp'] = extract_item(child, ['date'])\n",
    "\n",
    "        item['links'] = extract_items(child, ['link'])\n",
    "        item['links'] += extract_items(child, ['docs'])\n",
    "        \n",
    "        return tidy_dict(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'service': {'endpoints': [{'actionable': 'NOPE',\n",
       "    'parameters': [{'name': 'format', 'type': 'opensearch'},\n",
       "     {'name': 'option', 'type': 'com_search'},\n",
       "     {'name': 'view', 'type': 'remind'}],\n",
       "    'protocol': 'application/opensearchdescription+xml',\n",
       "    'template': 'http://www.ceos.org/index.php?option=com_search&view=remind&format=opensearch'},\n",
       "   {'actionable': 'NOPE',\n",
       "    'parameters': [{'name': 'option', 'type': 'com_search'},\n",
       "     {'name': 'searchword', 'type': '{searchTerms}'}],\n",
       "    'protocol': 'text/html',\n",
       "    'template': 'http://www.ceos.org/index.php?option=com_search&searchword={searchTerms}'}],\n",
       "  'title': ['CEOS']}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### let's do stuff\n",
    "\n",
    "response = '''<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<OpenSearchDescription xmlns=\"http://a9.com/-/spec/opensearch/1.1/\">\n",
    "    <ShortName>CEOS</ShortName>\n",
    "    <Description/>\n",
    "    <InputEncoding>UTF-8</InputEncoding>\n",
    "    <Image type=\"image/vnd.microsoft.icon\" width=\"16\" height=\"16\"\n",
    "        >http://www.ceos.org/templates/oceanwaves/favicon.ico</Image>\n",
    "    <Url type=\"application/opensearchdescription+xml\" rel=\"self\"\n",
    "        template=\"http://www.ceos.org/index.php?option=com_search&amp;view=remind&amp;format=opensearch\"/>\n",
    "    <Url type=\"text/html\"\n",
    "        template=\"http://www.ceos.org/index.php?option=com_search&amp;searchword={searchTerms}\"/>\n",
    "</OpenSearchDescription>'''\n",
    "\n",
    "url = 'http://www.ceos.org/index.php?option=com_search&amp;view=remind&amp;format=opensearch'\n",
    "identity = {\"protocol\": \"OpenSearch\", \"service\": {\"name\": \"DescriptionDocument\"}}\n",
    "\n",
    "reader = OpenSearchReader(identity, response.replace('\\n', ''), url)\n",
    "reader.parse()\n",
    "reader.description\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'updated': '2014-06-21T19:49:32-06:00', 'links': [{'href': 'https://www.sciencebase.gov/catalog/item/5287d495e4b03b89f6f1a088.atom', 'rel': 'self'}, {'href': 'https://www.sciencebase.gov/catalog/item/5287d495e4b03b89f6f1a088', 'rel': ''}, {'href': 'https://www.sciencebase.gov/catalog/item/feedMap/5287d495e4b03b89f6f1a088', 'rel': 'related'}], 'subjects': ['geology', 'geologic maps', 'surficial geologic units', 'unconsolidated deposits', 'geospatial datasets', 'geoscientificInformation', 'Canyonlands National Park', 'Utah', 'Druid Arch 7.5-minute quadrangle', 'The Loop 7.5-minute quadrangle', '49037 = San Juan'], 'published': '2013-11-16T13:24:53-07:00', 'title': 'Surficial Geologic Map of The Loop and Druid Arch Quadrangles,Canyonlands National Park, Utah', 'id': 'https://www.sciencebase.gov/catalog/item/5287d495e4b03b89f6f1a088', 'contents': [{'content': '<div> This geologic map is a product of a cooperative project between theU.S. Geological Survey and the U.S. National Park Service to providegeologic information about this part of Canyonlands National Park, Utah.This digital map database contains bedrock data from previously publisheddata that has been modified by the author. New mapping of the surficialdeposits represents the general distribution of surficial deposits of theDruid Arch and The Loop 7.5-minute quadrangles.</div>', 'type': 'html'}]}, {'updated': '2015-02-05T11:22:45-07:00', 'links': [{'href': 'https://www.sciencebase.gov/catalog/item/537e4b03e4b05ed6215c0bb5.atom', 'rel': 'self'}, {'href': 'https://www.sciencebase.gov/catalog/item/537e4b03e4b05ed6215c0bb5', 'rel': ''}, {'href': 'https://www.sciencebase.gov/catalog/item/feedMap/537e4b03e4b05ed6215c0bb5', 'rel': 'related'}], 'subjects': ['geology', 'geologic maps', 'surficial geologic units', 'unconsolidated deposits', 'geospatial datasets', 'geoscientificInformation', 'Canyonlands National Park', 'Utah', 'Druid Arch 7.5-minute quadrangle', 'The Loop 7.5-minute quadrangle', '49037 = San Juan'], 'published': '2014-05-22T13:07:47-06:00', 'title': 'Surficial Geologic Map of The Loop and Druid Arch Quadrangles,Canyonlands National Park, Utah', 'id': 'https://www.sciencebase.gov/catalog/item/537e4b03e4b05ed6215c0bb5', 'contents': [{'content': 'This geologic map is a product of a cooperative project between theU.S. Geological Survey and the U.S. National Park Service to providegeologic information about this part of Canyonlands National Park, Utah.This digital map database contains bedrock data from previously publisheddata that has been modified by the author. New mapping of the surficialdeposits represents the general distribution of surficial deposits of theDruid Arch and The Loop 7.5-minute quadrangles.', 'type': 'html'}]}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'children': {'items': [{'contents': [{'content': '<div> This geologic map is a product of a cooperative project between theU.S. Geological Survey and the U.S. National Park Service to providegeologic information about this part of Canyonlands National Park, Utah.This digital map database contains bedrock data from previously publisheddata that has been modified by the author. New mapping of the surficialdeposits represents the general distribution of surficial deposits of theDruid Arch and The Loop 7.5-minute quadrangles.</div>',\n",
       "      'type': 'html'}],\n",
       "    'id': 'https://www.sciencebase.gov/catalog/item/5287d495e4b03b89f6f1a088',\n",
       "    'links': [{'href': 'https://www.sciencebase.gov/catalog/item/5287d495e4b03b89f6f1a088.atom',\n",
       "      'rel': 'self'},\n",
       "     {'href': 'https://www.sciencebase.gov/catalog/item/5287d495e4b03b89f6f1a088',\n",
       "      'rel': ''},\n",
       "     {'href': 'https://www.sciencebase.gov/catalog/item/feedMap/5287d495e4b03b89f6f1a088',\n",
       "      'rel': 'related'}],\n",
       "    'published': '2013-11-16T13:24:53-07:00',\n",
       "    'subjects': ['geology',\n",
       "     'geologic maps',\n",
       "     'surficial geologic units',\n",
       "     'unconsolidated deposits',\n",
       "     'geospatial datasets',\n",
       "     'geoscientificInformation',\n",
       "     'Canyonlands National Park',\n",
       "     'Utah',\n",
       "     'Druid Arch 7.5-minute quadrangle',\n",
       "     'The Loop 7.5-minute quadrangle',\n",
       "     '49037 = San Juan'],\n",
       "    'title': 'Surficial Geologic Map of The Loop and Druid Arch Quadrangles,Canyonlands National Park, Utah',\n",
       "    'updated': '2014-06-21T19:49:32-06:00'},\n",
       "   {'contents': [{'content': 'This geologic map is a product of a cooperative project between theU.S. Geological Survey and the U.S. National Park Service to providegeologic information about this part of Canyonlands National Park, Utah.This digital map database contains bedrock data from previously publisheddata that has been modified by the author. New mapping of the surficialdeposits represents the general distribution of surficial deposits of theDruid Arch and The Loop 7.5-minute quadrangles.',\n",
       "      'type': 'html'}],\n",
       "    'id': 'https://www.sciencebase.gov/catalog/item/537e4b03e4b05ed6215c0bb5',\n",
       "    'links': [{'href': 'https://www.sciencebase.gov/catalog/item/537e4b03e4b05ed6215c0bb5.atom',\n",
       "      'rel': 'self'},\n",
       "     {'href': 'https://www.sciencebase.gov/catalog/item/537e4b03e4b05ed6215c0bb5',\n",
       "      'rel': ''},\n",
       "     {'href': 'https://www.sciencebase.gov/catalog/item/feedMap/537e4b03e4b05ed6215c0bb5',\n",
       "      'rel': 'related'}],\n",
       "    'published': '2014-05-22T13:07:47-06:00',\n",
       "    'subjects': ['geology',\n",
       "     'geologic maps',\n",
       "     'surficial geologic units',\n",
       "     'unconsolidated deposits',\n",
       "     'geospatial datasets',\n",
       "     'geoscientificInformation',\n",
       "     'Canyonlands National Park',\n",
       "     'Utah',\n",
       "     'Druid Arch 7.5-minute quadrangle',\n",
       "     'The Loop 7.5-minute quadrangle',\n",
       "     '49037 = San Juan'],\n",
       "    'title': 'Surficial Geologic Map of The Loop and Druid Arch Quadrangles,Canyonlands National Park, Utah',\n",
       "    'updated': '2015-02-05T11:22:45-07:00'}]}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../response_examples/opensearch_usgs_search_atom.xml', 'r') as f:\n",
    "    response = f.read().replace('\\\\\\n', '').replace('\\\\n', '').replace('\\n', '')\n",
    "    \n",
    "url = 'http://www.ceos.org/index.php?option=com_search&amp;view=remind&amp;format=opensearch'\n",
    "identity = {\"protocol\": \"OpenSearch\",\n",
    "    \"resultset\": {\n",
    "        \"dialect\": \"ATOM\"\n",
    "    }}\n",
    "\n",
    "reader = OpenSearchReader(identity, response.replace('\\n', ''), url)\n",
    "reader.parse()\n",
    "reader.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
