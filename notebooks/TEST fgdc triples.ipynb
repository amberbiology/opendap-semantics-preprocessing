{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quick checks of the new fgdc triples structure\n",
    "and maybe the triples (yes the triples, whatevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from semproc.parser import Parser\n",
    "from semproc.preprocessors.metadata_preprocessors import FgdcItemReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the proto-triples example\n",
    "with open('../response_examples/fgdc_proto_example_1.xml', 'r') as f:\n",
    "    response = f.read()\n",
    "\n",
    "# this shouldn't be necessary but cargo-culting here is fine by me.\n",
    "response = response.replace('\\\\\\n', '').replace('\\r\\n', '').replace('\\\\r', '').replace('\\\\n', '').replace('\\n', '')\n",
    "response = response.decode('utf-8', errors='replace').encode('unicode_escape') \n",
    "    \n",
    "url = 'https://bluehub.jrc.ec.europa.eu/erddap/metadata/fgdc/xml/noaa_pfeg_d543_8870_bc7f_fgdc.xml'\n",
    "identities = [\n",
    "    {\"protocol\": \"FGDC\", \n",
    "     \"metadata\": {\n",
    "            \"version\": [\"FGDC Content Standards for Digital Geospatial Metadata, FGDC-STD-001-1998\"], \n",
    "            \"name\": \"FGDC\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "parser = Parser(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# execute the parse (this one takes xml)\n",
    "reader = FgdcItemReader(parser.xml, url, '2015-06-20T20:22:00.643Z')\n",
    "description = reader.parse_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catalog_record': {'conformsTo': 'http://www.ngdc.noaa.gov/metadata/published/xsd/ngdcSchema/schema.xsd',\n",
       "  'harvestDate': '2015-06-20T20:22:00.643Z',\n",
       "  'object_id': 'urn:sha:f65fb3d1efeee860adbbb53d3a20e80e1f50fe625e4887155f196a92',\n",
       "  'relationships': [{'object_id': 'urn:sha:d5a3a66150264cfeca19322b996a1cbf202b15966e0f1e41b99ae991',\n",
       "    'relate': 'primaryTopic'}],\n",
       "  'url': 'https://bluehub.jrc.ec.europa.eu/erddap/metadata/fgdc/xml/noaa_pfeg_d543_8870_bc7f_fgdc.xml'},\n",
       " 'dataset': {'abstract': 'Navy Global Environmental Model (NAVGEM) is a global numerical weather prediction computer model. It replaced NOGAPS as the prime model in the middle of February 2013 at the Navy Fleet Numerical Meteorology and Oceanography Center (FNMOC) Weather model synoptic site. [Wikipedia]',\n",
       "  'identifier': 'https://bluehub.jrc.ec.europa.eu:noaa_pfeg_d543_8870_bc7f',\n",
       "  'object_id': 'urn:sha:d5a3a66150264cfeca19322b996a1cbf202b15966e0f1e41b99ae991',\n",
       "  'relationships': [{'object_id': 'urn:sha:f65fb3d1efeee860adbbb53d3a20e80e1f50fe625e4887155f196a92',\n",
       "    'relate': 'description'},\n",
       "   {'object_id': 'urn:uuid:8abe5047-0549-4b0e-8377-32e02734a915',\n",
       "    'relate': 'publisher'},\n",
       "   {'object_id': 'urn:sha:c8e6a320278b006f2f7b6b293fcab0331b77b84cfd63b47408537e85',\n",
       "    'relate': 'relation'},\n",
       "   {'object_id': 'urn:sha:be86d663bf9cb281de03aff9ae3fa51c6728075d344e55ca3a376514',\n",
       "    'relate': 'relation'},\n",
       "   {'object_id': 'urn:sha:8df1983f1f9573be5059558fd20d718d9550720c873eb8930cdc6735',\n",
       "    'relate': 'relation'},\n",
       "   {'object_id': 'urn:uuid:90c52c87-6cf6-4083-899b-981e4fc0ea45',\n",
       "    'relate': 'conformsTo'},\n",
       "   {'object_id': 'urn:uuid:a35775fc-0819-4129-a5ce-be1ff76f7494',\n",
       "    'relate': 'conformsTo'}],\n",
       "  'spatial_extent': {'east': '180.0',\n",
       "   'north': '90.0',\n",
       "   'south': '-90.0',\n",
       "   'west': '-180.0',\n",
       "   'wkt': 'POLYGON ((-180 -90,-180 90,180 90,180 -90,-180 -90))'},\n",
       "  'temporal_extent': {'endDate': '2014-08-13T00:00:00',\n",
       "   'startDate': '2013-02-15T00:00:00'},\n",
       "  'title': 'Navy Global Environmental Model (NAVGEM), 0.5 degree, Pressure MSL'},\n",
       " 'keywords': [{'object_id': 'urn:uuid:90c52c87-6cf6-4083-899b-981e4fc0ea45',\n",
       "   'terms': ['conversion',\n",
       "    'data',\n",
       "    'fnmoc',\n",
       "    'level',\n",
       "    'mean',\n",
       "    'navg',\n",
       "    'observed',\n",
       "    'pressure',\n",
       "    'sea',\n",
       "    'theortically',\n",
       "    'value'],\n",
       "   'thesaurus': 'Uncontrolled',\n",
       "   'type': 'theme'},\n",
       "  {'object_id': 'urn:uuid:a35775fc-0819-4129-a5ce-be1ff76f7494',\n",
       "   'terms': ['time', 'latitude', 'longitude'],\n",
       "   'thesaurus': 'CF Standard Name Table v27',\n",
       "   'type': 'theme'}],\n",
       " 'publisher': {'location': 'Ispra, VA, Italy',\n",
       "  'name': 'ERDDAP, version 1.60, at Maritime Affairs Unit of the Joint Research Centre',\n",
       "  'object_id': 'urn:uuid:8abe5047-0549-4b0e-8377-32e02734a915'},\n",
       " 'webpages': [{'object_id': 'urn:sha:c8e6a320278b006f2f7b6b293fcab0331b77b84cfd63b47408537e85',\n",
       "   'url': 'https://bluehub.jrc.ec.europa.eu/erddap/griddap/noaa_pfeg_d543_8870_bc7f.html'},\n",
       "  {'object_id': 'urn:sha:be86d663bf9cb281de03aff9ae3fa51c6728075d344e55ca3a376514',\n",
       "   'url': 'https://bluehub.jrc.ec.europa.eu/erddap/griddap/noaa_pfeg_d543_8870_bc7f.graph'},\n",
       "  {'object_id': 'urn:sha:8df1983f1f9573be5059558fd20d718d9550720c873eb8930cdc6735',\n",
       "   'url': 'https://bluehub.jrc.ec.europa.eu/erddap/wms/noaa_pfeg_d543_8870_bc7f/request'}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'theme', 'thesaurus': 'GCMD Keyword Thesaurus', 'object_id': 'urn:uuid:571dbe03-82a3-45a5-a0b8-192ec92b5002'}\n",
      "@prefix bcube: <http://purl.org/BCube/#> .\n",
      "@prefix bibo: <http://purl.org/ontology/bibo/#> .\n",
      "@prefix dc: <http://purl.org/dc/elements/1.1/> .\n",
      "@prefix dcat: <http://www.w3.org/TR/vocab-dcat/#> .\n",
      "@prefix dct: <http://purl.org/dc/terms/> .\n",
      "@prefix esip: <http://purl.org/esip/#> .\n",
      "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
      "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
      "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "@prefix vcard: <http://www.w3.org/TR/vcard-rdf/#> .\n",
      "@prefix vivo: <http://vivo.ufl.edu/ontology/vivo-ufl/#> .\n",
      "@prefix xml: <http://www.w3.org/XML/1998/namespace> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "<urn:sha:1646ee4d39fef9103635b9fcced8c474c560b673eb4813fd3ed0cba6> owl:a bibo:WebPage ;\n",
      "    vcard:hasURL \"http://datafedwiki.wustl.edu/index.php/RETRO_ANTHRO\" .\n",
      "\n",
      "<urn:sha:c88e518c724b8df319323b39478db676c15242a3c3b20ec29d5e13fc> owl:a bibo:WebPage ;\n",
      "    vcard:hasURL \"http://retro.enes.org/emissions/\" .\n",
      "\n",
      "<urn:sha:f7c002c73d636b5a5c47462ad316b5552d0f86539eb4e9143a5e361f> vivo:harvestDate \"2015-03-06T01:17:15.778Z\" ;\n",
      "    owl:a dcat:CatalogRecord ;\n",
      "    vcard:hasURL \"http://catalog.data.gov/harvest/object/0e11e207-c223-4953-85be-5b5f9a5132d2/original\" ;\n",
      "    foaf:primaryTopic <urn:uuid:549d6457-ad36-41a6-b04c-fd1e27baa8e3> .\n",
      "\n",
      "<urn:uuid:549d6457-ad36-41a6-b04c-fd1e27baa8e3> dc:conformsTo <urn:uuid:571dbe03-82a3-45a5-a0b8-192ec92b5002> ;\n",
      "    dc:description <urn:sha:f7c002c73d636b5a5c47462ad316b5552d0f86539eb4e9143a5e361f>,\n",
      "        \"Anthropogenic and vegetation fire emissions data were generated monthly covering a period of 1960 to 2000.  Anthropogenic emissions in the RETRO inventory are derived from the TNO database and the VERITAS inventory of international ship traffic emissions.\" ;\n",
      "    dc:relation <urn:sha:1646ee4d39fef9103635b9fcced8c474c560b673eb4813fd3ed0cba6>,\n",
      "        <urn:sha:c88e518c724b8df319323b39478db676c15242a3c3b20ec29d5e13fc> ;\n",
      "    dc:spatial \"POLYGON ((-180 -90,-180 90,180 90,180 -90,-180 -90))\" ;\n",
      "    dct:identifier \"\" ;\n",
      "    dct:title \"World Emission RETRO ANTHRO\" ;\n",
      "    esip:eastBound \"180.0\"^^xsd:float ;\n",
      "    esip:endDate \"None\"^^xsd:date ;\n",
      "    esip:northBound \"90.0\"^^xsd:float ;\n",
      "    esip:southBound \"-90.0\"^^xsd:float ;\n",
      "    esip:startDate \"None\"^^xsd:date ;\n",
      "    esip:westBound \"-180.0\"^^xsd:float ;\n",
      "    owl:a dcat:Dataset ;\n",
      "    dcat:publisher <urn:uuid:f8b0bcdb-ca24-4827-a219-b7de8eba33d6> .\n",
      "\n",
      "<urn:uuid:571dbe03-82a3-45a5-a0b8-192ec92b5002> dc:hasType \"theme\" ;\n",
      "    dc:partOf \"GCMD Keyword Thesaurus\" ;\n",
      "    owl:a bcube:thesaurusSubset .\n",
      "\n",
      "<urn:uuid:f8b0bcdb-ca24-4827-a219-b7de8eba33d6> dc:location \"TBD\" ;\n",
      "    owl:a dcat:publisher ;\n",
      "    foaf:name \"RETRO\" .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's play with triples\n",
    "\n",
    "import rdflib\n",
    "import hashlib\n",
    "from uuid import uuid4\n",
    "from rdflib import Graph, Literal, RDF, RDFS, Namespace, URIRef\n",
    "from rdflib.namespace import DC, DCTERMS, FOAF, XSD, OWL\n",
    "\n",
    "\n",
    "class Grapher():\n",
    "    def __init__(self):\n",
    "        self.graph = Graph()\n",
    "        self._bind_namespaces()\n",
    "    \n",
    "    # some faked namespaces\n",
    "    _ontology_uris = {\n",
    "        'bcube': 'http://purl.org/BCube/#',\n",
    "        'vcard': 'http://www.w3.org/TR/vcard-rdf/#',\n",
    "        'esip': 'http://purl.org/esip/#',\n",
    "        'vivo': 'http://vivo.ufl.edu/ontology/vivo-ufl/#',\n",
    "        'bibo': 'http://purl.org/ontology/bibo/#',\n",
    "        'dcat': 'http://www.w3.org/TR/vocab-dcat/#',\n",
    "        'dc': str(DC),\n",
    "        'dct': str(DCTERMS),\n",
    "        'foaf': str(FOAF),\n",
    "        'xsd': str(XSD),\n",
    "        'owl': str(OWL)\n",
    "    }\n",
    "    \n",
    "    def _bind_namespaces(self):\n",
    "        # bind our lovely fake namespaces\n",
    "        for prefix, uri in self._ontology_uris.iteritems():\n",
    "            self.graph.bind(prefix, uri)\n",
    "\n",
    "    def generate_predicate(self, prefix, name):\n",
    "        return Namespace(self._ontology_uris[prefix])[name]\n",
    "\n",
    "    def identify_prefix(self, predicate):\n",
    "        # this is, granted, a lesson in technical debt.\n",
    "        debt = {\n",
    "            \"dc\": [\"description\", \"conformsTo\", \"relation\"],\n",
    "            \"dcat\": [\"publisher\"],\n",
    "            \"foaf\": [\"primaryTopic\"]\n",
    "        }\n",
    "        \n",
    "        for k, v in debt.iteritems():\n",
    "            if predicate in v:\n",
    "                return k\n",
    "        return ''\n",
    "            \n",
    "    def create_resource(self, resource_prefix, resource_type, identifier=''):\n",
    "        # make a thing with a uuid as a urn\n",
    "        # and just assign it to type if it's not overridden\n",
    "        identifier = identifier if identifier else uuid4().urn\n",
    "        resource = self.graph.resource(identifier)\n",
    "        ref = Namespace(self._ontology_uris[resource_prefix])[resource_type]\n",
    "        resource.add(OWL.a, URIRef(ref))\n",
    "        return resource\n",
    "\n",
    "    def _process_catalog(self, entity):\n",
    "        catalog_record = self.create_resource('dcat', 'CatalogRecord', entity['object_id'])\n",
    "        catalog_record.add(self.generate_predicate('vcard', 'hasURL'), Literal(entity['url']))\n",
    "        catalog_record.add(self.generate_predicate('vivo', 'harvestDate'), Literal(entity['harvestDate']))\n",
    "        if entity['conformsTo']:\n",
    "            catalog_record.add(DC.conformsTo, Literal(entity['conformsTo']))\n",
    "        \n",
    "        for relationship in entity['relationships']:\n",
    "            # so. current object, verb, id of object, existence unknown\n",
    "            self.relates.append((catalog_record, relationship['relate'], relationship['object_id']))\n",
    "            \n",
    "    def _process_dataset(self, entity):\n",
    "        dataset = self.create_resource('dcat', 'Dataset', entity['object_id'])\n",
    "        dataset.add(DCTERMS.identifier, Literal(entity['identifier']))\n",
    "        dataset.add(DCTERMS.title, Literal(entity['title']))\n",
    "        dataset.add(DC.description, Literal(entity['abstract']))\n",
    "        \n",
    "        if 'temporal_extent' in entity:\n",
    "            # NOTE: make these iso 8601 first\n",
    "            begdate = entity['temporal_extent'].get('startDate')\n",
    "            enddate = entity['temporal_extent'].get('endDate')\n",
    "\n",
    "            dataset.add(self.generate_predicate('esip', 'startDate'), Literal(begdate, datatype=XSD.date))\n",
    "            dataset.add(self.generate_predicate('esip', 'endDate'), Literal(enddate, datatype=XSD.date))\n",
    "\n",
    "        if 'spatial_extent' in entity:\n",
    "            dataset.add(DC.spatial, Literal(entity['spatial_extent']['wkt']))\n",
    "\n",
    "            # a small not good thing.\n",
    "            dataset.add(self.generate_predicate('esip', 'westBound'),\n",
    "                        Literal(float(entity['spatial_extent']['west']), datatype=XSD.float))\n",
    "\n",
    "            dataset.add(self.generate_predicate('esip', 'eastBound'),\n",
    "                        Literal(float(entity['spatial_extent']['east']), datatype=XSD.float))\n",
    "\n",
    "            dataset.add(self.generate_predicate('esip', 'southBound'),\n",
    "                        Literal(float(entity['spatial_extent']['south']), datatype=XSD.float))\n",
    "\n",
    "            dataset.add(self.generate_predicate('esip', 'northBound'),\n",
    "                        Literal(float(entity['spatial_extent']['north']), datatype=XSD.float))\n",
    "        \n",
    "        for relationship in entity['relationships']:\n",
    "            self.relates.append((dataset, relationship['relate'], relationship['object_id']))\n",
    "        \n",
    "    def _process_keywords(self, entity):\n",
    "        for keywords in entity:\n",
    "            keyset = self.create_resource('bcube', 'thesaurusSubset', keywords['object_id'])\n",
    "            if 'type' in keywords:\n",
    "                keyset.add(DC.hasType, Literal(keywords['type']))\n",
    "            if 'thesaurus' in keywords:\n",
    "                keyset.add(DC.partOf, Literal(keywords['thesaurus']))\n",
    "\n",
    "            try:\n",
    "                for term in keywords['terms']:\n",
    "                    keyset.add(self.generate_predicate('bcube', 'hasValue'), Literal(term))\n",
    "            except:\n",
    "                print keywords\n",
    "        \n",
    "    def _process_publisher(self, entity):\n",
    "        publisher = self.create_resource('dcat', 'publisher', entity['object_id'])\n",
    "        publisher.add(DC.location, Literal(entity['location']))\n",
    "        publisher.add(FOAF.name, Literal(entity['name']))\n",
    "        \n",
    "    def _process_webpages(self, entity):\n",
    "        for webpage in entity:\n",
    "            relation = self.create_resource('bibo', 'WebPage', webpage['object_id'])\n",
    "            relation.add(self.generate_predicate('vcard', 'hasURL'), Literal(webpage['url']))\n",
    "    \n",
    "    def serialize(self):\n",
    "        return self.graph.serialize(format='turtle')\n",
    "    \n",
    "    def graphalize(self, doc):\n",
    "        # not a word\n",
    "        # so from our json.\n",
    "        \n",
    "        # this. is. idk. an ordering thing. i suspect the graph borks\n",
    "        # when you try to add a triple for a non-existent object.\n",
    "        # years of experience. also, i am wrong - it will add anything.\n",
    "        self.relates = []\n",
    "        for entity_type, entity in doc.iteritems():\n",
    "            if entity_type == 'catalog_record':\n",
    "                self._process_catalog(entity)\n",
    "            elif entity_type == 'dataset':\n",
    "                self._process_dataset(entity)\n",
    "            elif entity_type == 'publisher':\n",
    "                self._process_publisher(entity)\n",
    "            elif entity_type == 'keywords':\n",
    "                self._process_keywords(entity)\n",
    "            elif entity_type == 'webpages':\n",
    "                self._process_webpages(entity)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        for resource, verb, object_id in self.relates:\n",
    "            resource.add(\n",
    "                self.generate_predicate(\n",
    "                    self.identify_prefix(verb), verb),\n",
    "                URIRef(object_id)\n",
    "            )\n",
    "        \n",
    "\n",
    "grapher = Grapher()    \n",
    "grapher.graphalize(description)\n",
    "print grapher.serialize()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generating local graphs for any fgdc response\n",
    "\n",
    "i would note that we are not catching every fgdc through the identifier - not quite valid/complete responses are being dropped. this generates roughly three thousand graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so let's make a new temp class for the harvest + parser parts\n",
    "# and this is not what we really want to do. probably.\n",
    "\n",
    "class Fgdc():\n",
    "    def __init__(self, doc):\n",
    "        self.url = doc.get('url', '')\n",
    "        self.response = self._prep_response(doc.get('raw_content', ''))\n",
    "        self.harvested = doc.get('tstamp', '')  # except i did not carry this through in the clean task\n",
    "        self._prep_parser()\n",
    "        \n",
    "    def _prep_parser(self):\n",
    "        parser = Parser(self.response)\n",
    "        self.reader = FgdcItemReader(parser.xml, self.url, self.harvested)\n",
    "    \n",
    "    def _prep_response(self, response):\n",
    "        # this shouldn't be necessary but cargo-culting here is fine by me.\n",
    "        response = response.encode('unicode_escape')\n",
    "        response = response.replace('\\\\\\n', '').replace('\\r\\n', '').replace('\\\\r', '').replace('\\\\n', '').replace('\\n', '')\n",
    "        return response.decode('utf-8', errors='replace').encode('unicode_escape') \n",
    "        \n",
    "    def parse(self):\n",
    "        return self.reader.parse_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw_content_md5': '001165c4b4bb8c02389f1a216a71ab69', 'source_url': 'http://hdsc.nws.noaa.gov/hdsc/pfds/meta/na14_vol5_gu_grid_metadata.xml'}\n",
      "{'raw_content_md5': '001346b24aca9c7d741c5cc0e3186681', 'source_url': 'http://catalog.data.gov/harvest/object/9a4ecbd7-643d-49f6-805a-0c6b7928783b/original'}\n",
      "{'raw_content_md5': '043cdbc3ef789a66713ed7d49d5f6a7f', 'source_url': 'http://www.ncddc.noaa.gov/approved_recs/nos_de/csc/rcsd/Chlorophyll/CMECS_Eupohotic_metadata.xml'}\n",
      "{'raw_content_md5': '00305fb6ea2c40eab991e028575627f8', 'source_url': 'http://catalog.data.gov/harvest/object/c7572f49-2da3-4bcf-a5a8-38003a55aad4/original'}\n",
      "{'raw_content_md5': '00396a0e800619e08f206376cabae988', 'source_url': 'http://www.coris.noaa.gov/metadata/records/xml/Grammanik_45el_45az_10m.xml'}\n",
      "{'raw_content_md5': '004831165a780e1ba35eebd9a39c2acf', 'source_url': 'http://gstore.unm.edu/apps/rgis/datasets/41b157be-ae1f-4011-97d1-d975618fe7fd/metadata/FGDC-STD-001-1998.xml'}\n",
      "{'raw_content_md5': '004cd8a41674143268af65d47043bfc3', 'source_url': 'http://catalog.data.gov/harvest/object/0e11e207-c223-4953-85be-5b5f9a5132d2/original'}\n",
      "wtf <theme xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:smw=\"http://smw.ontoware.org/2005/smw#\" xmlns:attribute=\"http://wiki.esipfed.org/index.php/Special:URIResolver/Attribute-3A\"><themekt>GCMD Keyword Thesaurus</themekt><themekey/></theme>\n",
      "{'raw_content_md5': '005f630392b3754e7a67034da04ea0bf', 'source_url': 'http://data.denvergov.org/download/gis/instream_sampling_sites/metadata/instream_sampling_sites.xml'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "docs_dir = '/Users/sscott/Documents/working_bits/solr_20150707/docs'\n",
    "triples_dir = '/Users/sscott/Documents/tmp/triples/'\n",
    "\n",
    "with open('data/small_harvest_fgdc_as_md5.csv', 'rb') as csvfile:\n",
    "    cr = csv.DictReader(csvfile, delimiter=\"|\")\n",
    "\n",
    "    cnt = 0\n",
    "    for row in cr:\n",
    "        print row\n",
    "        \n",
    "        if cnt > 5:\n",
    "            break\n",
    "        \n",
    "        url = row['source_url'].strip().replace('\"', '')\n",
    "        md5 = row['raw_content_md5'].strip().replace('\"', '')\n",
    "        \n",
    "        # NOTE: this is obviously not correct but time constraints\n",
    "        # harvest_date = '2015-07-07T00:00:00.000Z'\n",
    "        \n",
    "        # go open the cleaned up version by md5\n",
    "        filepath = os.path.join(docs_dir, md5 + '.json')\n",
    "        if not os.path.exists(filepath):\n",
    "            continue\n",
    "        \n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "            \n",
    "        fgdc = Fgdc(data)\n",
    "        description = fgdc.parse()\n",
    "        \n",
    "        grapher = Grapher()\n",
    "        grapher.graphalize(description)\n",
    "        ttl = grapher.serialize()\n",
    "        \n",
    "        with open(os.path.join(triples_dir, md5 + '.ttl'), 'w') as f:\n",
    "            f.write(ttl)\n",
    "        \n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Atmosphere > Air Quality > Emissions',\n",
       " 'Human Dimensions > Environmental Impacts',\n",
       " 'Atmosphere > Air Quality > Carbon Monoxide',\n",
       " 'Atmosphere > Air Quality > Nitrogen Oxides',\n",
       " 'Atmosphere > Air Quality > Sulfur Oxides']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import etree\n",
    "from semproc.xml_utils import extract_items\n",
    "\n",
    "xml = etree.fromstring('''<metadata xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:smw=\"http://smw.ontoware.org/2005/smw#\" xmlns:attribute=\"http://wiki.esipfed.org/index.php/Special:URIResolver/Attribute-3A\">\n",
    "<keywords>\n",
    "            <theme>\n",
    "                <themekt>GCMD Keyword Thesaurus</themekt>\n",
    "                <themekey><![CDATA[Atmosphere > Air Quality > Emissions]]></themekey>\n",
    "                <themekey><![CDATA[Human Dimensions > Environmental Impacts]]></themekey>\n",
    "                <themekey><![CDATA[Atmosphere > Air Quality > Carbon Monoxide]]></themekey>\n",
    "                <themekey><![CDATA[Atmosphere > Air Quality > Nitrogen Oxides]]></themekey>\n",
    "                <themekey><![CDATA[Atmosphere > Air Quality > Sulfur Oxides]]></themekey>\n",
    "            </theme>\n",
    "            <place>\n",
    "                <placekt>Uncontrolled Keywords</placekt>\n",
    "                <placekey>US</placekey>\n",
    "            </place>\n",
    "        </keywords></metadata>\n",
    "''')\n",
    "\n",
    "extract_items(xml, ['keywords', 'theme', 'themekey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
