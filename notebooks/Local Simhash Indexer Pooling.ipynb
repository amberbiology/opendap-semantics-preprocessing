{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the local simhash pooled index tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a shallow fork of simhashindex (https://github.com/liangsun/simhash/blob/master/simhash/__init__.py)\n",
    "# what we need to do, in a db-free test env, is retain more than the string and the hash of\n",
    "# the string (we have the sha of the source, the string, and the simhash of the string)\n",
    "# and a way to exclude a sha from the result set - find equivalent objects in another response\n",
    "\n",
    "# we are now trying pathos\n",
    "\n",
    "from simhash import Simhash\n",
    "# from multiprocessing import Pool\n",
    "# import multiprocessing\n",
    "# from collections import defaultdict\n",
    "import pathos.multiprocessing as mp\n",
    "\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "import os\n",
    "\n",
    "# import copy_reg\n",
    "# import types\n",
    "# from copy_reg import pickle\n",
    "# from types import MethodType\n",
    "\n",
    "\n",
    "class IndexBucket(object):\n",
    "    def __init__(self, f=64, k=2):\n",
    "        self.bucket = {}\n",
    "        self.k = k\n",
    "        self.f = f\n",
    "\n",
    "    def get_near_dups(self, simhash):\n",
    "        \"\"\"\n",
    "        `simhash` is an instance of Simhash\n",
    "        return a list of obj_id (pipe-delimited string of sha|text|distance)\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        ans = set()\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            dups = self.bucket.get(key, set())\n",
    "\n",
    "            for dup in dups:\n",
    "                sim2, obj_blob = dup.split(',', 1)\n",
    "                sim2 = Simhash(long(sim2, 16), self.f)\n",
    "\n",
    "                d = simhash.distance(sim2)\n",
    "                if d <= self.k:\n",
    "                    ans.add('{0}|{1}'.format(obj_blob, d))\n",
    "        return list(ans)\n",
    "    \n",
    "    def add(self, obj_id, obj_str, simhash):\n",
    "        \"\"\"\n",
    "        `obj_id` is a string\n",
    "        `simhash` is an instance of Simhash\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            v = '%x,%s|%s' % (simhash.value, obj_id, obj_str)\n",
    "\n",
    "            self.bucket.setdefault(key, set())\n",
    "            self.bucket[key].add(v)\n",
    "\n",
    "    def delete(self, obj_id, obj_str, simhash):\n",
    "        \"\"\"\n",
    "        `obj_id` is a string\n",
    "        `simhash` is an instance of Simhash\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            v = '%x,%s|%s' % (simhash.value, obj_id, obj_str)\n",
    "\n",
    "            if v in self.bucket.get(key, set()):\n",
    "                self.bucket[key].remove(v)\n",
    "    \n",
    "    @property\n",
    "    def offsets(self):\n",
    "        \"\"\"\n",
    "        You may optimize this method according to <http://www.wwwconference.org/www2007/papers/paper215.pdf>\n",
    "        \"\"\"\n",
    "        return [self.f // (self.k + 1) * i for i in range(self.k + 1)]\n",
    "\n",
    "    def get_keys(self, simhash):\n",
    "        for i, offset in enumerate(self.offsets):\n",
    "            m = (i == len(self.offsets) - 1 and 2 ** (self.f - offset) - 1 or 2 ** (self.offsets[i + 1] - offset) - 1)\n",
    "            c = simhash.value >> offset & m\n",
    "            yield '%x:%x' % (c, i)\n",
    "\n",
    "    def bucket_size(self):\n",
    "        return len(self.bucket)\n",
    "    \n",
    "\n",
    "class HashIndex(object):\n",
    "    # this is now a wrapper for the pooled indexing process\n",
    "    def mapper(hashindex, arr):\n",
    "        hashindex._map(arr)\n",
    "    \n",
    "    def _partition(self):\n",
    "        # chunk out our store into smaller bits\n",
    "        breakpoint = int(round(len(self.store) / self.workers + 0.5))\n",
    "        i = 0\n",
    "        while i < len(self.store):\n",
    "            yield self.store[i:i+breakpoint]\n",
    "            i += breakpoint\n",
    "\n",
    "    def _map(arr):\n",
    "        # make a bucket and compare the simhash to\n",
    "        # that local set, return if distance < k\n",
    "        bucket = IndexBucket(k=self.k, f=self.f)\n",
    "\n",
    "        for i, q in enumerate(arr):\n",
    "            bucket.add(*q)\n",
    "\n",
    "        # do the comparison\n",
    "        near_dupes = bucket.get_near_dups(self.simhash)\n",
    "\n",
    "        # return tuples of object strings, distance scores\n",
    "        return [tuple(n.split('|')) for n in near_dupes]\n",
    "\n",
    "    def _reduce(mappings):\n",
    "        # join and sort asc\n",
    "        near_dupes = list(chain.from_iterable(mappings))\n",
    "\n",
    "        return sorted(near_dupes, key=lambda x: itemgetter(1))\n",
    "    \n",
    "    def get_near_dupes(self, simhash):\n",
    "        # run the pooled process against the store\n",
    "        pool = mp.Pool(processes=self.workers,)\n",
    "        \n",
    "        # pool.map args workaround #1\n",
    "        self.simhash = simhash\n",
    "        \n",
    "        partitions = self._partition()\n",
    "        \n",
    "        # with pathos\n",
    "        mapped_buckets = pool.map(_map, partitions)\n",
    "        \n",
    "#         # won't pickle\n",
    "#         #mapped_buckets = pool.map(_map, partitions)\n",
    "        \n",
    "# #         mapped_buckets = pool.map(_calling, args=(self, '_map', partitions))\n",
    "# #         try the async i guess\n",
    "#         async_results = [pool.apply_async(mapper, args=(self, (p,))) for p in partitions]\n",
    "#         pool.close()\n",
    "#         map(multiprocessing.pool.ApplyResult.wait, async_results)\n",
    "        \n",
    "#         print async_results\n",
    "        \n",
    "#         mapped_buckets = [r.get() for r in async_results]       \n",
    "        \n",
    "        # for multiple args and a helper that's gone, didn't work (??)\n",
    "        #mapped_buckets = pool.map(_map_helper, izip(partitions, repeat((simhash, self.k, self.f))))\n",
    "        near_dupes = self._reduce(mapped_buckets)\n",
    "        \n",
    "        return near_dupes\n",
    "        \n",
    "    def __init__(self, objs, f=64, k=2):\n",
    "        \"\"\"\n",
    "        `objs` is a list of (sha, source obj (str), simhash)\n",
    "        obj_id is a string, simhash is an instance of Simhash\n",
    "        `f` is the same with the one for Simhash\n",
    "        `k` is the tolerance\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.f = f\n",
    "        \n",
    "        self.workers= 10\n",
    "        #count = len(objs)\n",
    "        \n",
    "        self.store = objs \n",
    "    \n",
    "def _calling(instance, name, args=(), kwargs=None):\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "    return getattr(instance, name)(*args, **kwargs)\n",
    "    \n",
    "    \n",
    "# # pickling things\n",
    "# def _pickle_method(method):\n",
    "#     func_name = method.im_func.__name__\n",
    "#     obj = method.im_self\n",
    "#     cls = method.im_class\n",
    "#     return _unpickle_method, (func_name, obj, cls)\n",
    "\n",
    "# def _unpickle_method(func_name, obj, cls):\n",
    "#     for cls in cls.mro():\n",
    "#         try:\n",
    "#             func = cls.__dict__[func_name]\n",
    "#         except KeyError:\n",
    "#             pass\n",
    "#         else:\n",
    "#             break\n",
    "            \n",
    "# copy_reg.pickle(types.MethodType, _pickle_method, _unpickle_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test item =  649e490e 13504959716208044071\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name '_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c33316a36232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'test item = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnear_dupes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_near_dupes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mnear_dupes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e0fe1ccaf8eb>\u001b[0m in \u001b[0;36mget_near_dupes\u001b[0;34m(self, simhash)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# with pathos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mmapped_buckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m#         # won't pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name '_map' is not defined"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4  \n",
    "\n",
    "# the test is uuid similarity (which is junk but auto-generated junk)\n",
    "big_list = [str(uuid4()).split('-')[0]for i in xrange(0, 1000)]\n",
    "big_list = [(b, Simhash(b)) for b in big_list]\n",
    "\n",
    "index = HashIndex(big_list)\n",
    "test_item = big_list[1]\n",
    "print 'test item = ', test_item[0], test_item[1].value\n",
    "\n",
    "near_dupes = index.get_near_dupes(test_item[1])\n",
    "near_dupes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
