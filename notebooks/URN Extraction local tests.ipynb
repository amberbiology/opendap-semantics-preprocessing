{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "import dateutil.parser as dateparser\n",
    "import urlparse\n",
    "import urllib\n",
    "from itertools import chain, izip\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = '''<metadata>\n",
    "    <title id=\"\"></title>\n",
    "    <!-- check for the json as attribute -->\n",
    "    <sometext human_address=\"{&quot;address&quot;:&quot;1801 Hawthorn Ln.&quot;,&quot;city&quot;:&quot;West Chicago&quot;,&quot;state&quot;:&quot;IL&quot;,&quot;zip&quot;:&quot;60185&quot;}\"></sometext>\n",
    "    <!-- scale things that are easy -->\n",
    "    <sometext>Public Land Survey System data were collected from 1:500,000 and 1:1,000,000 State base maps for the western United States.</sometext>\n",
    "    <!-- scale things that are less easy -->\n",
    "    <sometext>#1:100,000-scale base maps</sometext>\n",
    "    <!-- mostly the same. -->\n",
    "    <sometext>In all, 85 corrections were made using 1:100,000-scale base maps.</sometext>\n",
    "    <!-- mostly the same but also tilde. -->\n",
    "    <sometext>\"~1:63,000-scale\"</sometext>\n",
    "    <sometext>Here's some stuff with a url http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp in it.</sometext>\n",
    "    <sometext>Here's some stuff with a url &lt;http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp&gt; in it.</sometext>\n",
    "    <sometext>Here's some stuff with a url http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp} in it.</sometext>\n",
    "    <sometext>Here's some stuff with a url (http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp) in it.</sometext>\n",
    "    <sometext></sometext>\n",
    "    <sometext>ISO 19115-2:2009(E)</sometext>\n",
    "    \n",
    "    <!-- fun with bad dates -->\n",
    "    <somedate>9999-01-01T00:00:00</somedate>\n",
    "    <somedate>0001-01-01T00:00:00</somedate>\n",
    "    <somedate>NaN-01-01T00:00:00</somedate>\n",
    "    <somedate>\"1999-01-01T00:00:00\"</somedate>\n",
    "    <somedate>00:35:00</somedate>\n",
    "    <somedate>\"00:35:00\"</somedate>\n",
    "    <somedate></somedate>\n",
    "    \n",
    "    <!-- exclude by tags or tag-related things -->\n",
    "    <someelement>\n",
    "        <CI_DateTypeCode codeList=\"http://www.isotc211.org/2005/resources/Codelist/gmxCodelists.xml#CI_DateTypeCode\" codeListValue=\"publication\"></CI_DateTypeCode>\n",
    "    </someelement>\n",
    "    <someelement template=\"http://www.blob.com\"></someelement>\n",
    "    <someelement><Value>blob:ThisIsNot:A:URN</Value></someelement>\n",
    "    <someelement schemaLocation=\"http://www.blob.com/schema.xsd\"></someelement>\n",
    "    \n",
    "    <!-- things with urls in them -->\n",
    "    <someurl>&lt;URL: plss_la_usgs_2003.gif&gt;</someurl>\n",
    "    <someurl>\"http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp?catalog=http:/ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml&amp;dataset=las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface\"</someurl>\n",
    "    <someurl>http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp .</someurl>\n",
    "    <someurl>(http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp)</someurl>\n",
    "    <someurl>(http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp):</someurl>\n",
    "    <someurl></someurl>\n",
    "    <someurl></someurl>\n",
    "    \n",
    "    <someurn>WWW:SPARQL:1.1</someurn>\n",
    "</metadata>'''\n",
    "\n",
    "\n",
    "# content = '''<metadata>\n",
    "#     <someurl>\"http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp?catalog=http://ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml&amp;dataset=las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface\"</someurl>\n",
    "# </metadata>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicParser():\n",
    "    '''\n",
    "    not concerned about namespaces or querying\n",
    "\n",
    "    note: these could merge at some point\n",
    "    '''\n",
    "    def __init__(self, text, handle_html=False, include_html_hrefs=False):\n",
    "        self.text = text.encode('unicode_escape')\n",
    "        self.parser = etree.XMLParser(\n",
    "            remove_blank_text=True,\n",
    "            remove_comments=True,\n",
    "            recover=True,\n",
    "            remove_pis=True\n",
    "        )\n",
    "        self.handle_html = handle_html\n",
    "        self.include_html_hrefs = include_html_hrefs\n",
    "\n",
    "        self._parse()\n",
    "        \n",
    "        #print self.xml\n",
    "\n",
    "    def _parse(self):\n",
    "        try:\n",
    "            self.xml = etree.fromstring(self.text, parser=self.parser)\n",
    "        except Exception as ex:\n",
    "            print ex\n",
    "            raise\n",
    "\n",
    "    def _un_htmlify(self, text):\n",
    "        def _handle_bad_html(s):\n",
    "            pttn = re.compile('<|>')\n",
    "            return pttn.sub(' ', s)\n",
    "\n",
    "        soup = BeautifulSoup(text.strip())\n",
    "\n",
    "        # get all of the text and any a/@href values\n",
    "        texts = [_handle_bad_html(t.strip('\"')) for t in soup.find_all(text=True)]\n",
    "        if self.include_html_hrefs:\n",
    "            texts += [unquote(a['href']) for a in soup.find_all('a') if 'href' in a.attrs]\n",
    "\n",
    "        try:\n",
    "            text = ' '.join(texts)\n",
    "        except Exception as ex:\n",
    "            print ex\n",
    "            raise\n",
    "        return text\n",
    "\n",
    "    def strip_text(self, exclude_tags=[]):\n",
    "        def _extract_tag(t):\n",
    "            if not t:\n",
    "                return\n",
    "            return t.split('}')[-1]\n",
    "\n",
    "        def _taggify(e):\n",
    "            tags = [e.tag] + [m.tag for m in e.iterancestors()]\n",
    "            tags.reverse()\n",
    "\n",
    "            try:\n",
    "                return [_extract_tag(t) for t in tags]\n",
    "            except:\n",
    "                return []\n",
    "\n",
    "        for elem in self.xml.iter():\n",
    "            t = elem.text.strip() if elem.text else ''\n",
    "            tags = _taggify(elem)\n",
    "\n",
    "            if [e for e in exclude_tags if e in tags]:\n",
    "                continue\n",
    "\n",
    "            if t:\n",
    "                if self.handle_html and (\n",
    "                        (t.startswith('<') and t.endswith('>'))\n",
    "                        or ('<' in t or '>' in t)):\n",
    "                    t = self._un_htmlify(t)\n",
    "                if t:\n",
    "                    yield ('/'.join(tags), t)\n",
    "\n",
    "            for k, v in elem.attrib.iteritems():\n",
    "                if v.strip():\n",
    "                    v = next(iter(BeautifulSoup(v.strip())), '')\n",
    "                    if v:\n",
    "                        yield ('/'.join(tags + ['@' + _extract_tag(k)]), v.find(\"p\").string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_pattern_set = [\n",
    "     ('url', re.compile(ur\"((?:(?:https?|ftp|http)://)(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:.\\d{1,3}){3})(?!(?:169.254|192.168)(?:.\\d{1,3}){2})(?!172.(?:1[6-9]|2\\d|3[0-1])(?:.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-*)*[a-z\\\\u00a1-\\\\uffff0-9]+)(?:.(?:[a-z\\\\u00a1-\\\\uffff0-9]-*)*[a-z\\\\u00a1-\\\\uffff0-9]+)*(?:.(?:[a-z\\\\u00a1-\\\\uffff]{2,})))(?::\\d{2,5})?(?:/\\S*)?)\", re.IGNORECASE)),\n",
    "    # a urn that isn't a url\n",
    "    ('urn', re.compile(ur\"(?![http://])(?![https://])(?![ftp://])(([a-z0-9.\\S][a-z0-9-.\\S]{0,}\\S:{1,2}\\S)+[a-z0-9()+,\\-.=@;$_!*'%/?#]+)\", re.IGNORECASE)),\n",
    "    # ('urn', re.compile(ur\"\\burn:[a-z0-9][a-z0-9-]{0,31}:[a-z0-9()+,\\-.:=@;$_!*'%/?#]+\", re.IGNORECASE)),\n",
    "    ('uuid', re.compile(ur'([a-f\\d]{8}(-[a-f\\d]{4}){3}-[a-f\\d]{12}?)', re.IGNORECASE)),\n",
    "    ('doi', re.compile(ur\"(10[.][0-9]{4,}(?:[/][0-9]+)*/(?:(?![\\\"&\\\\'])\\S)+)\", re.IGNORECASE)),\n",
    "    ('md5', re.compile(ur\"([a-f0-9]{32})\", re.IGNORECASE))\n",
    "]\n",
    "\n",
    "_rule_set = [\n",
    "    ('uri', 'fileIdentifier/CharacterString'),  # ISO\n",
    "    ('uri', 'identifier/*/code/CharacterString'),\n",
    "    ('uri', 'dataSetURI/CharacterString'),\n",
    "    ('uri', 'parentIdentifier/CharacterString'),\n",
    "    ('uri', 'Entry_ID'),  # DIF\n",
    "    ('uri', 'dc/identifier'),  # DC\n",
    "    ('basic', 'Layer/Name'),  # WMS\n",
    "    ('basic', 'dataset/@ID'),  # THREDDS\n",
    "    ('uri', '@URI'),  # ddi\n",
    "    ('uri', '@IDNo')  # ddi\n",
    "]\n",
    "\n",
    "def match(s, p):\n",
    "    '''\n",
    "    extract from regex\n",
    "    '''\n",
    "    m = re.search(p, s)\n",
    "    return m.group(0) if m else ''\n",
    "\n",
    "def unquote(url):\n",
    "    return urllib.unquote(url)\n",
    "\n",
    "\n",
    "def break_url(url):\n",
    "    parts = urlparse.urlparse(url)\n",
    "\n",
    "    url = urlparse.urlunparse((\n",
    "        parts.scheme,\n",
    "        parts.netloc,\n",
    "        parts.path,\n",
    "        None, None, None\n",
    "    ))\n",
    "\n",
    "    params = urlparse.parse_qs(parts.query)\n",
    "    values = list(chain.from_iterable((params.values())))\n",
    "\n",
    "    return url, ' '.join(values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the new workflow\n",
    "'''\n",
    "extract urls from the text\n",
    "split into base and values\n",
    "extract urls from values\n",
    "\n",
    "\n",
    "store as sha, text, type, method, id\n",
    "'''\n",
    "\n",
    "class Identifier(object):\n",
    "    pass\n",
    "\n",
    "class IdentifierExtractor(object):\n",
    "    def __init__(self, source_url, source_xml_as_str):\n",
    "        self.source_url = source_url\n",
    "        self.source_xml_as_str = source_xml_as_str\n",
    "        \n",
    "        self.identifieds = []\n",
    "        self.texts = []\n",
    "        self.seen_texts = []\n",
    "        \n",
    "        self._parse()\n",
    "    \n",
    "    def _parse(self):\n",
    "        try:\n",
    "            parser = BasicParser(self.source_xml_as_str, True, True)\n",
    "        except Exception as ex:\n",
    "            print ex\n",
    "        if not parser or parser.xml is None:\n",
    "            raise Exception('failed to parse')\n",
    "        \n",
    "        for tag, txt in parser.strip_text():\n",
    "            self.texts.append((tag, txt))\n",
    "            \n",
    "    def _strip_punctuation(self, text):\n",
    "        terminal_punctuation = '(){}[].,~|\":'\n",
    "        text = text.strip(terminal_punctuation)\n",
    "        return text.strip()\n",
    "\n",
    "    def _strip_dates(self, text):\n",
    "        # this should still make it an invalid date\n",
    "        # text = text[3:] if text.startswith('NaN') else text\n",
    "        try:\n",
    "            d = dateparser.parse(text)\n",
    "            return ''\n",
    "        except ValueError:\n",
    "            return text\n",
    "                \n",
    "    def _strip_scales(self, text):\n",
    "        scale_pttn = ur\"(1:[\\d]{0,}(,[\\d]{3}){1,})\"\n",
    "        m = match(text, scale_pttn)\n",
    "        if m:\n",
    "            return ''\n",
    "        return text\n",
    "\n",
    "    def _strip_excludes(self, text):\n",
    "        if any(e.lower() in text.lower() for e in excludes):\n",
    "            return ''\n",
    "        return text\n",
    "        \n",
    "    def _tidy_text(self, text):\n",
    "        text = self._strip_punctuation(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "        \n",
    "        text = self._strip_scales(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "    \n",
    "        text = self._strip_dates(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    def _extract_url(self, text):\n",
    "        pttn = re.compile(ur\"((?:(?:https?|ftp|http)://)(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:.\\d{1,3}){3})(?!(?:169.254|192.168)(?:.\\d{1,3}){2})(?!172.(?:1[6-9]|2\\d|3[0-1])(?:.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-*)*[a-z\\\\u00a1-\\\\uffff0-9]+)(?:.(?:[a-z\\\\u00a1-\\\\uffff0-9]-*)*[a-z\\\\u00a1-\\\\uffff0-9]+)*(?:.(?:[a-z\\\\u00a1-\\\\uffff]{2,})))(?::\\d{2,5})?(?:/\\S*)?)\",\n",
    "                          re.IGNORECASE)\n",
    "        m = match(text, pttn)\n",
    "        if not m:\n",
    "            return '', []\n",
    "        \n",
    "        space_pattern = re.compile(' ')\n",
    "        if space_pattern.subn(' ', m)[1] > 0:\n",
    "            # i actually don't know if this is the right index. huh.\\\n",
    "            m = m.split(' ')[0]\n",
    "\n",
    "        url = unquote(m)\n",
    "        base_url, values = break_url(url)\n",
    "        \n",
    "        # return the original extracted url, and the values plus \n",
    "        # the base_url for more extracting\n",
    "        return url, values.split(' ') + [base_url]\n",
    "\n",
    "    def _extract_identifiers(self, text):\n",
    "        for pattern_type, pattern in _pattern_set:\n",
    "            m = match(text, pattern)\n",
    "            if not m:\n",
    "                continue\n",
    "            yield pattern_type, m\n",
    "      \n",
    "    def _check_identifieds(self, extracted):\n",
    "        return len([a for a in self.identifieds if a[4] == extracted]) > 0\n",
    "    \n",
    "    def _check_seens(self, extracted):\n",
    "        return len([a for a in self.seen_texts if a == extracted]) > 0\n",
    "    \n",
    "    def process_text(self):\n",
    "        while self.texts:\n",
    "            tag, text = self.texts.pop()\n",
    "            if self._check_seens(text) or not text.strip():\n",
    "                continue\n",
    "                \n",
    "            print 'LOOP: ', len(self.texts)\n",
    "            \n",
    "            print 'STARTING: ', text\n",
    "            try:\n",
    "                j = json.loads(text)\n",
    "                j.keys()  # it will decode a quoted string without error\n",
    "                # print 'wtf', j\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            url, values = self._extract_url(text)\n",
    "            if url:\n",
    "                print 'VALUES A:', len(values)\n",
    "                print 'VALUES: ', values\n",
    "            \n",
    "            values = [v for v in values if not self._check_seens(v)]\n",
    "            if url:\n",
    "                print 'VALUES B:', len(values)\n",
    "            self.texts += list(iter(izip([tag]* len(values), values)))\n",
    "            print 'ADDING: ', len(self.texts)\n",
    "            \n",
    "            if url and not self._check_identifieds(url) and not self._check_seens(url):\n",
    "                self.identifieds.append((tag, text, 'url', 'regex', url))\n",
    "                self.seen_texts.append(url)\n",
    "            \n",
    "            # now run the OTHER regex\n",
    "            for match_type, match_text in self._extract_identifiers(text):\n",
    "                if match_text and not self._check_identifieds(match_text):\n",
    "                    self.identifieds.append((tag, text, match_type, 'regex', self._tidy_text(match_text)))\n",
    "                if not self._check_seens(match_text):\n",
    "                    self.texts.append((tag, match_text))\n",
    "                    self.seen_texts.append(match_text)\n",
    "                    print 'MATCH ADDING: ', len(self.texts)\n",
    "                \n",
    "        \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOP:  27\n",
      "STARTING:  WWW:SPARQL:1.1\n",
      "ADDING:  27\n",
      "MATCH ADDING:  28\n",
      "LOOP:  26\n",
      "STARTING:  (http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp):\n",
      "VALUES A: 2\n",
      "VALUES:  ['', 'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp']\n",
      "VALUES B: 2\n",
      "ADDING:  28\n",
      "MATCH ADDING:  29\n",
      "LOOP:  24\n",
      "STARTING:  http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp .\n",
      "VALUES A: 2\n",
      "VALUES:  ['', 'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp']\n",
      "VALUES B: 1\n",
      "ADDING:  25\n",
      "LOOP:  23\n",
      "STARTING:  \"http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp?catalog=http:/ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml&dataset=las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface\"\n",
      "VALUES A: 3\n",
      "VALUES:  ['http:/ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml', 'las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface', 'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp']\n",
      "VALUES B: 2\n",
      "ADDING:  25\n",
      "MATCH ADDING:  26\n",
      "LOOP:  24\n",
      "STARTING:  las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface\n",
      "ADDING:  24\n",
      "LOOP:  23\n",
      "STARTING:  http:/ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml\n",
      "ADDING:  23\n",
      "LOOP:  22\n",
      "STARTING:  http://www.blob.com/schema.xsd\n",
      "VALUES A: 2\n",
      "VALUES:  ['', u'http://www.blob.com/schema.xsd']\n",
      "VALUES B: 2\n",
      "ADDING:  24\n",
      "LOOP:  21\n",
      "STARTING:  blob:ThisIsNot:A:URN\n",
      "ADDING:  21\n",
      "MATCH ADDING:  22\n",
      "LOOP:  20\n",
      "STARTING:  http://www.blob.com\n",
      "VALUES A: 2\n",
      "VALUES:  ['', u'http://www.blob.com']\n",
      "VALUES B: 2\n",
      "ADDING:  22\n",
      "LOOP:  19\n",
      "STARTING:  publication\n",
      "ADDING:  19\n",
      "LOOP:  18\n",
      "STARTING:  http://www.isotc211.org/2005/resources/Codelist/gmxCodelists.xml#CI_DateTypeCode\n",
      "VALUES A: 2\n",
      "VALUES:  ['', u'http://www.isotc211.org/2005/resources/Codelist/gmxCodelists.xml']\n",
      "VALUES B: 2\n",
      "ADDING:  20\n",
      "LOOP:  19\n",
      "STARTING:  http://www.isotc211.org/2005/resources/Codelist/gmxCodelists.xml\n",
      "VALUES A: 2\n",
      "VALUES:  ['', u'http://www.isotc211.org/2005/resources/Codelist/gmxCodelists.xml']\n",
      "VALUES B: 2\n",
      "ADDING:  21\n",
      "LOOP:  17\n",
      "STARTING:  \\n\n",
      "ADDING:  17\n",
      "LOOP:  16\n",
      "STARTING:  \"00:35:00\"\n",
      "ADDING:  16\n",
      "MATCH ADDING:  17\n",
      "LOOP:  15\n",
      "STARTING:  00:35:00\n",
      "ADDING:  15\n",
      "MATCH ADDING:  16\n",
      "LOOP:  14\n",
      "STARTING:  \"1999-01-01T00:00:00\"\n",
      "ADDING:  14\n",
      "MATCH ADDING:  15\n",
      "LOOP:  13\n",
      "STARTING:  NaN-01-01T00:00:00\n",
      "ADDING:  13\n",
      "MATCH ADDING:  14\n",
      "LOOP:  12\n",
      "STARTING:  0001-01-01T00:00:00\n",
      "ADDING:  12\n",
      "MATCH ADDING:  13\n",
      "LOOP:  11\n",
      "STARTING:  9999-01-01T00:00:00\n",
      "ADDING:  11\n",
      "MATCH ADDING:  12\n",
      "LOOP:  10\n",
      "STARTING:  ISO 19115-2:2009(E)\n",
      "ADDING:  10\n",
      "MATCH ADDING:  11\n",
      "LOOP:  9\n",
      "STARTING:  Here's some stuff with a url (http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp) in it.\n",
      "VALUES A: 2\n",
      "VALUES:  ['', 'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp']\n",
      "VALUES B: 1\n",
      "ADDING:  10\n",
      "LOOP:  8\n",
      "STARTING:  Here's some stuff with a url http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp} in it.\n",
      "VALUES A: 2\n",
      "VALUES:  ['', 'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp']\n",
      "VALUES B: 1\n",
      "ADDING:  9\n",
      "LOOP:  7\n",
      "STARTING:  Here's some stuff with a url   in it.\n",
      "ADDING:  7\n",
      "LOOP:  6\n",
      "STARTING:  Here's some stuff with a url http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp in it.\n",
      "VALUES A: 2\n",
      "VALUES:  ['', 'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp']\n",
      "VALUES B: 1\n",
      "ADDING:  7\n",
      "MATCH ADDING:  8\n",
      "LOOP:  5\n",
      "STARTING:  \"~1:63,000-scale\"\n",
      "ADDING:  5\n",
      "MATCH ADDING:  6\n",
      "LOOP:  4\n",
      "STARTING:  In all, 85 corrections were made using 1:100,000-scale base maps.\n",
      "ADDING:  4\n",
      "LOOP:  3\n",
      "STARTING:  #1:100,000-scale base maps\n",
      "ADDING:  3\n",
      "MATCH ADDING:  4\n",
      "LOOP:  2\n",
      "STARTING:  Public Land Survey System data were collected from 1:500,000 and 1:1,000,000 State base maps for the western United States.\n",
      "ADDING:  2\n",
      "LOOP:  1\n",
      "STARTING:  {\"address\":\"1801 Hawthorn Ln.\",\"city\":\"West Chicago\",\"state\":\"IL\",\"zip\":\"60185\"}\n",
      "LOOP:  0\n",
      "STARTING:  \\n\n",
      "ADDING:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('metadata/someurn', 'WWW:SPARQL:1.1', 'urn', 'regex', 'WWW:SPARQL:1.1'),\n",
       " ('metadata/someurl',\n",
       "  '(http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp):',\n",
       "  'url',\n",
       "  'regex',\n",
       "  'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp'),\n",
       " ('metadata/someurl',\n",
       "  '(http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp):',\n",
       "  'urn',\n",
       "  'regex',\n",
       "  'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp'),\n",
       " ('metadata/someurl',\n",
       "  '\"http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp?catalog=http:/ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml&dataset=las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface\"',\n",
       "  'url',\n",
       "  'regex',\n",
       "  'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp?catalog=http:/ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml&dataset=las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface'),\n",
       " ('metadata/someurl',\n",
       "  '\"http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp?catalog=http:/ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml&dataset=las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface\"',\n",
       "  'urn',\n",
       "  'regex',\n",
       "  'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp?catalog=http:/ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml'),\n",
       " ('metadata/someelement/@schemaLocation',\n",
       "  u'http://www.blob.com/schema.xsd',\n",
       "  'url',\n",
       "  'regex',\n",
       "  u'http://www.blob.com/schema.xsd'),\n",
       " ('metadata/someelement/Value',\n",
       "  'blob:ThisIsNot:A:URN',\n",
       "  'urn',\n",
       "  'regex',\n",
       "  'blob:ThisIsNot:A:URN'),\n",
       " ('metadata/someelement/@template',\n",
       "  u'http://www.blob.com',\n",
       "  'url',\n",
       "  'regex',\n",
       "  u'http://www.blob.com'),\n",
       " ('metadata/someelement/CI_DateTypeCode/@codeList',\n",
       "  u'http://www.isotc211.org/2005/resources/Codelist/gmxCodelists.xml#CI_DateTypeCode',\n",
       "  'url',\n",
       "  'regex',\n",
       "  u'http://www.isotc211.org/2005/resources/Codelist/gmxCodelists.xml#CI_DateTypeCode'),\n",
       " ('metadata/someelement/CI_DateTypeCode/@codeList',\n",
       "  u'http://www.isotc211.org/2005/resources/Codelist/gmxCodelists.xml',\n",
       "  'url',\n",
       "  'regex',\n",
       "  u'http://www.isotc211.org/2005/resources/Codelist/gmxCodelists.xml'),\n",
       " ('metadata/somedate', '\"00:35:00\"', 'urn', 'regex', '00:35:00'),\n",
       " ('metadata/somedate',\n",
       "  '\"1999-01-01T00:00:00\"',\n",
       "  'urn',\n",
       "  'regex',\n",
       "  '1999-01-01T00:00:00'),\n",
       " ('metadata/somedate',\n",
       "  'NaN-01-01T00:00:00',\n",
       "  'urn',\n",
       "  'regex',\n",
       "  'NaN-01-01T00:00:00'),\n",
       " ('metadata/somedate',\n",
       "  '0001-01-01T00:00:00',\n",
       "  'urn',\n",
       "  'regex',\n",
       "  '0001-01-01T00:00:00'),\n",
       " ('metadata/somedate',\n",
       "  '9999-01-01T00:00:00',\n",
       "  'urn',\n",
       "  'regex',\n",
       "  '9999-01-01T00:00:00'),\n",
       " ('metadata/sometext',\n",
       "  'ISO 19115-2:2009(E)',\n",
       "  'urn',\n",
       "  'regex',\n",
       "  '19115-2:2009(E'),\n",
       " ('metadata/sometext',\n",
       "  \"Here's some stuff with a url (http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp) in it.\",\n",
       "  'urn',\n",
       "  'regex',\n",
       "  'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp'),\n",
       " ('metadata/sometext',\n",
       "  \"Here's some stuff with a url http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp in it.\",\n",
       "  'url',\n",
       "  'regex',\n",
       "  'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp in it'),\n",
       " ('metadata/sometext', '\"~1:63,000-scale\"', 'urn', 'regex', '1:63,000-scale'),\n",
       " ('metadata/sometext',\n",
       "  '#1:100,000-scale base maps',\n",
       "  'urn',\n",
       "  'regex',\n",
       "  '#1:100,000-scale')]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor = IdentifierExtractor('http://www.example.com/some/stuff?uid=urn:234:MyThing&x=nothing', content)\n",
    "extractor.process_text()\n",
    "\n",
    "extractor.identifieds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp\n",
      "\n",
      "http:/ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface\n"
     ]
    }
   ],
   "source": [
    "s = 'http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp?catalog=http:/ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml&amp;dataset=las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface'\n",
    "url, values = break_url(s)\n",
    "\n",
    "print url\n",
    "print\n",
    "print values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'unicode' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-da4282160c9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'unicode' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "s = \"http://ferret.pmel.noaa.gov/thredds/view/ToolsUI.jnlp?catalog=http://ferret.pmel.noaa.gov/thredds/catalog/las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface_Level/catalog.xml&dataset=las/NOAA-CIRES-CDC-CDC_Derived_NCEP_Reanalysis_Products_Surface\"\n",
    "pttn = re.compile(ur\"((?:(?:https?|ftp|http)://)(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:.\\d{1,3}){3})(?!(?:169.254|192.168)(?:.\\d{1,3}){2})(?!172.(?:1[6-9]|2\\d|3[0-1])(?:.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-*)*[a-z\\\\u00a1-\\\\uffff0-9]+)(?:.(?:[a-z\\\\u00a1-\\\\uffff0-9]-*)*[a-z\\\\u00a1-\\\\uffff0-9]+)*(?:.(?:[a-z\\\\u00a1-\\\\uffff]{2,})))(?::\\d{2,5})?(?:/\\S*)?)\",\n",
    "                          re.IGNORECASE)\n",
    "match(s, pttn)\n",
    "\n",
    "\n",
    "c = json.loads('\"%s\"' % s)\n",
    "c.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE THIS STUFF RIGHT NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-47a232baa317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'{\"address\":\"1801 Hawthorn Ln.\",\"city\":\"West Chicago\",\"state\":\"IL\",\"zip\":\"60185\"}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup('http://www.example.com/some/stuff?uid=urn:234:MyThing&x=nothing')\n",
    "soup.find(\"p\").string\n",
    "\n",
    "json.loads(u'{\"address\":\"1801 Hawthorn Ln.\",\"city\":\"West Chicago\",\"state\":\"IL\",\"zip\":\"60185\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tidy(text):\n",
    "    terminal_punctuation = '(){}[].,~|\":'\n",
    "    text = text.strip(terminal_punctuation)\n",
    "    \n",
    "    print 'remove punctuation: ', text\n",
    "\n",
    "    text = text[3:] if text.startswith('NaN') else text\n",
    "    \n",
    "    print 'remove nan: ', text\n",
    "\n",
    "    # check for scale?\n",
    "    scale_pttn = ur\"(1:[\\d]{0,}(,[\\d]{3}){1,})\"\n",
    "    m = match(text, scale_pttn)\n",
    "    if m:\n",
    "        print 'found scale: ', text, m\n",
    "        return ''\n",
    "\n",
    "    try:\n",
    "        d = dateparser.parse(text)\n",
    "        print 'parsed date: ', d\n",
    "        return ''\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove punctuation:  9999-01-01T00:00:00\n",
      "remove nan:  9999-01-01T00:00:00\n",
      "parsed date:  9999-01-01 00:00:00\n",
      "9999-01-01T00:00:00  <>  \n",
      "remove punctuation:  0001-01-01T00:00:00\n",
      "remove nan:  0001-01-01T00:00:00\n",
      "parsed date:  2001-01-01 00:00:00\n",
      "0001-01-01T00:00:00  <>  \n",
      "remove punctuation:  NaN-01-01T00:00:00\n",
      "remove nan:  -01-01T00:00:00\n",
      "parsed date:  2015-01-01 00:00:00\n",
      "NaN-01-01T00:00:00  <>  \n",
      "remove punctuation:  1999-01-01T00:00:00\n",
      "remove nan:  1999-01-01T00:00:00\n",
      "parsed date:  1999-01-01 00:00:00\n",
      "\"1999-01-01T00:00:00\"  <>  \n",
      "remove punctuation:  00:35:00\n",
      "remove nan:  00:35:00\n",
      "parsed date:  2015-05-22 00:35:00\n",
      "00:35:00  <>  \n",
      "remove punctuation:  00:35:00\n",
      "remove nan:  00:35:00\n",
      "parsed date:  2015-05-22 00:35:00\n",
      "\"00:35:00\"  <>  \n",
      "remove punctuation:  00:35:00\n",
      "remove nan:  00:35:00\n",
      "parsed date:  2015-05-22 00:35:00\n",
      "\"00:35:00\"):  <>  \n"
     ]
    }
   ],
   "source": [
    "# just the date checker thing\n",
    "\n",
    "some_dates = [\n",
    "    '9999-01-01T00:00:00',\n",
    "    '0001-01-01T00:00:00',\n",
    "    'NaN-01-01T00:00:00',\n",
    "    '\"1999-01-01T00:00:00\"',\n",
    "    '00:35:00',\n",
    "    '\"00:35:00\"',\n",
    "    '\"00:35:00\"):'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for sd in some_dates:\n",
    "    nsd = tidy(sd)\n",
    "    print sd, ' <> ', nsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'AIRS.2003.01.01.L3.RetStd_H030.v6.0.12.0.G14121184349.hdf'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "text = '<value>&quot;AIRS.2003.01.01.L3.RetStd_H030.v6.0.12.0.G14121184349.hdf&quot;</value>'\n",
    "soup = BeautifulSoup(text)\n",
    "\n",
    "x = soup.find_all(text=True)[0]\n",
    "\n",
    "x.strip('\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'{\"address\":\"Venice\",\"city\":\"\",\"state\":\"CA\",\"zip\":\"90291\"}']\n"
     ]
    }
   ],
   "source": [
    "text = '<city>{&quot;address&quot;:&quot;Venice&quot;,&quot;city&quot;:&quot;&quot;,&quot;state&quot;:&quot;CA&quot;,&quot;zip&quot;:&quot;90291&quot;}</city>'\n",
    "\n",
    "soup = BeautifulSoup(text)\n",
    "x = soup.find_all(text=True)\n",
    "print x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
