{
 "metadata": {
  "name": "",
  "signature": "sha256:82fdd087ff455ee46c1958395066fa457b449bf91f666523ab739bebe37115a6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Notes on detecting text similarity between unparsed response text\n",
      "\n",
      "1. byte histogram/shannon's entropy?\n",
      "2. tf-idf (but issues with xml strings, etc)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# file_entropy.py\n",
      "#\n",
      "# Shannon Entropy of a file\n",
      "# = minimum average number of bits per character\n",
      "# required for encoding (compressing) the file\n",
      "#\n",
      "# So the theoretical limit (in bytes) for data compression:\n",
      "# Shannon Entropy of the file * file size (in bytes) / 8\n",
      "# (Assuming the file is a string of byte-size (UTF-8?) characters\n",
      "# because if not then the Shannon Entropy value would be different.)\n",
      "# FB - 201011291\n",
      "import math\n",
      "import os\n",
      "\n",
      "def calculate_entropy(filename):\n",
      "    # read the whole file into a byte array\n",
      "    with open(filename, \"rb\") as f:\n",
      "        byteArr = map(ord, f.read())\n",
      "\n",
      "    fileSize = len(byteArr)\n",
      "\n",
      "    # calculate the frequency of each byte value in the file\n",
      "    freqList = []\n",
      "    for b in range(256):\n",
      "        ctr = 0\n",
      "        for byte in byteArr:\n",
      "            if byte == b:\n",
      "                ctr += 1\n",
      "        freqList.append(float(ctr) / fileSize)\n",
      "\n",
      "    # Shannon entropy\n",
      "    ent = 0.0\n",
      "    for freq in freqList:\n",
      "        if freq > 0:\n",
      "            ent = ent + freq * math.log(freq, 2)\n",
      "    ent = -ent\n",
      "#     print 'Shannon entropy (min bits per byte-character):'\n",
      "#     print ent\n",
      "    return fileSize, freqList, ent\n",
      "\n",
      "files = ['../testdata/thredds/stellwagen_tsdata_catalog_partial.xml', \n",
      "         '../testdata/thredds/stellwagen_tsdata_catalog.xml',\n",
      "         '../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_minor.xml',\n",
      "         '../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_small.xml',\n",
      "         '../testdata/thredds/acdisc.xml']\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[0])\n",
      "print files[0], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[1])\n",
      "print files[1], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[2])\n",
      "print files[2], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[3])\n",
      "print files[3], filesize, ent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "../testdata/thredds/stellwagen_tsdata_catalog_partial.xml 1182 4.90021731832\n",
        "../testdata/thredds/stellwagen_tsdata_catalog.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7561 5.29295061933\n",
        "../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_minor.xml 1182 4.8983940163\n",
        "../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_small.xml 764 5.00913142639\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.metrics.pairwise import linear_kernel\n",
      "from sklearn import metrics\n",
      "\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "_stopwords = set(stopwords.words('english'))\n",
      "\n",
      "import re\n",
      "\n",
      "def strip_punctuation(text):\n",
      "    simple_pattern = r'[;|>+:=.,<?(){}`\\'\"]'\n",
      "    text = re.sub(simple_pattern, ' ', text)\n",
      "    return text.replace(\"/\", ' ')\n",
      "\n",
      "def tokenize(text):\n",
      "    return word_tokenize(text)\n",
      "    \n",
      "def remove_stopwords(words):\n",
      "    return ' '.join([w for w in words if w not in _stopwords and w])\n",
      "\n",
      "\n",
      "with open(files[0], 'r') as f:\n",
      "    original = f.read()\n",
      "    \n",
      "original = strip_punctuation(original)    \n",
      "original_words = tokenize(original)\n",
      "# print text0_words[0:10]\n",
      "original_words = remove_stopwords(original_words)\n",
      "original = ''.join([t for t in original_words if t])\n",
      "# print original\n",
      "\n",
      "with open(files[1], 'r') as f:\n",
      "    compare = f.read()\n",
      "    \n",
      "compare = strip_punctuation(compare)    \n",
      "compare_words = tokenize(compare)\n",
      "compare_words = remove_stopwords(compare_words)\n",
      "compare = ''.join([t for t in compare_words if t])\n",
      "# print text1\n",
      "\n",
      "tfidf_vectorizer = TfidfVectorizer()\n",
      "tfidf_matrix_trainer = tfidf_vectorizer.fit_transform([original, compare])\n",
      "\n",
      "cos_sim = cosine_similarity(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer)\n",
      "print cos_sim\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.         0.3130777]]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import glob\n",
      "import os\n",
      "\n",
      "## for the bag of words with xml element names (cheap structure)\n",
      "# this ignores html encoding cruft.\n",
      "\n",
      "def _strip_punctuation(text, simple_pattern=r'[;|>+:=#@%<?(){}`\\'\"]', replace_char=' '):\n",
      "    text = re.sub(simple_pattern, replace_char, text)\n",
      "    return text.replace(\"/\", ' ')\n",
      "def tokenize(text):\n",
      "    return word_tokenize(text)\n",
      "\n",
      "files = glob.glob('../testdata/solr_20150320/clean_20150325_p/*_cleaned.json')\n",
      "outdir = '../testdata/solr_20150320/bow_xml'\n",
      "for f in files:\n",
      "    with open(f, 'r') as g:\n",
      "        data = json.loads(g.read())\n",
      "\n",
      "    content = data['content']\n",
      "    digest = data['digest']\n",
      "    content = _strip_punctuation(content)\n",
      "    words = tokenize(content)\n",
      "    bag = remove_stopwords(words)\n",
      "    bag = _strip_punctuation(bag, r'[.,]', '')\n",
      "        \n",
      "    with open(os.path.join(outdir, '%s.txt' % digest), 'w') as g:\n",
      "        g.write(bag)\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## for the bag of words with any text element/attribute value but no tags\n",
      "from lxml import etree \n",
      "\n",
      "# taken from the Parser obj\n",
      "parser = etree.XMLParser(\n",
      "            encoding='utf-8',\n",
      "            ns_clean=True,\n",
      "            remove_blank_text=True,\n",
      "            remove_comments=True,\n",
      "            recover=True\n",
      "        )\n",
      "\n",
      "# modified to just return any text\n",
      "def find_nodes(xml):\n",
      "    nodes = []\n",
      "    for elem in xml.iter():\n",
      "        t = elem.text.strip() if elem.text else ''\n",
      "        tags = [elem.tag] + [e.tag for e in elem.iterancestors()]\n",
      "        tags.reverse()\n",
      "\n",
      "        # check for an assumed comment object (shouldn't be here at all)\n",
      "        if sum([isinstance(a, str) for a in tags]) != len(tags):\n",
      "            continue\n",
      "\n",
      "        att_texts = parse_node_attributes(elem)\n",
      "        nodes += [a for a in [t] + att_texts if a]\n",
      "    return nodes\n",
      "\n",
      "def parse_node_attributes(node):\n",
      "    if not node.attrib:\n",
      "        return []\n",
      "    return node.attrib.values() if node.attrib else []\n",
      "\n",
      "files = glob.glob('../testdata/solr_20150320/clean_20150325_p/*_cleaned.json')\n",
      "outdir = '../testdata/solr_20150320/bow_no_xml'\n",
      "for f in files:\n",
      "    with open(f, 'r') as g:\n",
      "        data = json.loads(g.read())\n",
      "\n",
      "    content = data['content'].encode('unicode_escape')\n",
      "    digest = data['digest']\n",
      "    \n",
      "    xml = etree.fromstring(content, parser=parser)\n",
      "    \n",
      "    if xml is None:\n",
      "        print 'NO XML: ', digest\n",
      "        continue\n",
      "    \n",
      "    nodes = find_nodes(xml)\n",
      "    content = ' '.join(nodes if nodes else [])\n",
      "    if not content:\n",
      "        print digest\n",
      "        continue\n",
      "    content = _strip_punctuation(content)\n",
      "    words = tokenize(content)\n",
      "    bag = remove_stopwords(words)\n",
      "    bag = _strip_punctuation(bag, r'[.,]', '')\n",
      "    \n",
      "    if bag:\n",
      "        with open(os.path.join(outdir, '%s.txt' % digest), 'w') as g:\n",
      "            try:\n",
      "                g.write(bag)\n",
      "            except UnicodeEncodeError:\n",
      "                g.write(bag.encode('ascii', 'ignore'))\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "04b219d820811a6cb1bc7af03b52b626\n",
        "0f161c6e18155f31a0c522d6434693ba"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12fc3bfc1b4f39a14032e9da8e592d11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "166d257ebc0e1be17d96e79c383c1b4b"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "184715fdf06bd9fc835dc62929e8d36f"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "27d44e4d5e96d3db5a8cf834b44b69a5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4c93c9e87b669cf74473b08be29483e2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5135ab69f05c6744087f2b2af19e1d15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5a77078bdfdb9408fb9155d7a9466f8f"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "61826bdd9307ff8d905712d3fbce7d29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6918ba9dc992d2c524730da594bd6dab"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ae8dc130b36b17e47127b1e69acba194"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NO XML: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " b5a4080d703e06e78402d86588c61291\n",
        "bf19615c45ddba4cbba611db639fe499"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "d888644c0238d7e21709bb7e80acccfd"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NO XML: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " db208a9b9ac4feed80a870ffc3490ef1\n",
        "f6a1bcedaa977efeb1b0b706c0659877"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "f7582357e363bbe21087b828272d5a47"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Looking for similar documents (without xml)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.metrics.pairwise import linear_kernel\n",
      "from sklearn import metrics\n",
      "import glob\n",
      "import os\n",
      "\n",
      "# for each file, run against every other.\n",
      "# so let's load them all into a 2d matrix, digest, bag of words\n",
      "bags = []\n",
      "files = glob.glob('../testdata/solr_20150320/bow_no_xml/*.txt')\n",
      "for f in files:\n",
      "    if '_similarities' in f:\n",
      "        continue\n",
      "    i = f.split('/')[-1].replace('.txt', '')\n",
      "    with open(f, 'r') as g:\n",
      "        bow = g.read()\n",
      "    \n",
      "    bags.append([i, bow])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# similarity scoring\n",
      "import time\n",
      "\n",
      "start_time = time.time()\n",
      "\n",
      "ptimes = []\n",
      "\n",
      "# do the tokenization of all the bags first \n",
      "# this is not a quick little thing\n",
      "all_identifiers = [b[0] for b in bags]\n",
      "all_bags = [b[1] for b in bags]\n",
      "\n",
      "tfidf_vectorizer = TfidfVectorizer()\n",
      "\n",
      "for i, bag in enumerate(bags):\n",
      "    if os.path.exists('../testdata/solr_20150320/bow_no_xml/%s_similarities.txt' % bag[0]):\n",
      "        continue\n",
      "    tf_start = time.time()\n",
      "    tf_identifiers = [all_identifiers[i]] + all_identifiers[:i-1] + all_identifiers[i:]\n",
      "    tf_data = [all_bags[i]] + all_bags[:i-1] + all_bags[i:]\n",
      "    \n",
      "    tfidf_matrix_trainer = tfidf_vectorizer.fit_transform(tf_data)\n",
      "    \n",
      "    cos_sim = cosine_similarity(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer)\n",
      "    \n",
      "    lk = linear_kernel(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer).flatten()\n",
      "\n",
      "    related_indices = cos_sim.argsort()[:-len(tf_data)-1:-1] \n",
      "    related_indices_with_lk_value = [r for r in related_indices[0] if lk[r] >= 0.1]\n",
      "    related_indices_with_lk_value.reverse()\n",
      "\n",
      "    related_set = [(lk[k], tf_data[k], tf_identifiers[k]) for k in related_indices_with_lk_value]\n",
      "    lines = []\n",
      "    for cs, b, d in related_set[1:]:\n",
      "        #print cs, b, d\n",
      "        lines.append('%s,%s' % (cs, d))\n",
      "    \n",
      "    ptimes.append(time.time() - tf_start)\n",
      "    if i % 50 == 0:\n",
      "        print time.time() - tf_start\n",
      "    \n",
      "    with open('../testdata/solr_20150320/bow_no_xml/%s_similarities.txt' % bag[0], 'w') as g:\n",
      "        g.write('\\n'.join(lines))\n",
      "\n",
      "print time.time() - start_time\n",
      "\n",
      "print 'average time:', sum(ptimes) / len(ptimes)\n",
      "print 'longest running:', max(ptimes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "47.7222130299\n",
        "48.5523808002"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "48.7056849003"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49.0535950661"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "52.9939260483"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51.7386889458"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-d23cc49a6b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_bags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mall_bags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mall_bags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtfidf_matrix_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcos_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix_trainer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_matrix_trainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \"\"\"\n\u001b[0;32m-> 1282\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                     \u001b[0;31m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "one: at least one response near 1.0 sim that has two entries in solr (two md5s) for one url (the tiger line rdf - the diff is that one element has a closing tag (`<tag thing=\"\"/>` vs `<tag thing=\"\"></tag>`)\n",
      "\n",
      "<img src=\"screenshots/rdf_compare_tigerlines.png\"/>\n",
      "\n",
      "hahaha. it's https vs http\n",
      "\n",
      "two: ddx files from some data collection and close in time/space have high similarity values\n",
      "\n",
      "three: what to do about binary base 64 image blobs\n",
      "\n",
      "four: what to do about numeric data (ddx, kml)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "From known sets - same metadata, different representations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}