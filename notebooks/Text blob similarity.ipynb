{
 "metadata": {
  "name": "",
  "signature": "sha256:7056b2226602023b0752d82851fc0f454643f8dc3da208ea494b1bc5aeadb623"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Notes on detecting text similarity between unparsed response text\n",
      "\n",
      "1. byte histogram/shannon's entropy?\n",
      "2. tf-idf (but issues with xml strings, etc)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# file_entropy.py\n",
      "#\n",
      "# Shannon Entropy of a file\n",
      "# = minimum average number of bits per character\n",
      "# required for encoding (compressing) the file\n",
      "#\n",
      "# So the theoretical limit (in bytes) for data compression:\n",
      "# Shannon Entropy of the file * file size (in bytes) / 8\n",
      "# (Assuming the file is a string of byte-size (UTF-8?) characters\n",
      "# because if not then the Shannon Entropy value would be different.)\n",
      "# FB - 201011291\n",
      "import math\n",
      "import os\n",
      "\n",
      "def calculate_entropy(filename):\n",
      "    # read the whole file into a byte array\n",
      "    with open(filename, \"rb\") as f:\n",
      "        byteArr = map(ord, f.read())\n",
      "\n",
      "    fileSize = len(byteArr)\n",
      "\n",
      "    # calculate the frequency of each byte value in the file\n",
      "    freqList = []\n",
      "    for b in range(256):\n",
      "        ctr = 0\n",
      "        for byte in byteArr:\n",
      "            if byte == b:\n",
      "                ctr += 1\n",
      "        freqList.append(float(ctr) / fileSize)\n",
      "\n",
      "    # Shannon entropy\n",
      "    ent = 0.0\n",
      "    for freq in freqList:\n",
      "        if freq > 0:\n",
      "            ent = ent + freq * math.log(freq, 2)\n",
      "    ent = -ent\n",
      "#     print 'Shannon entropy (min bits per byte-character):'\n",
      "#     print ent\n",
      "    return fileSize, freqList, ent\n",
      "\n",
      "files = ['../testdata/thredds/stellwagen_tsdata_catalog_partial.xml', \n",
      "         '../testdata/thredds/stellwagen_tsdata_catalog.xml',\n",
      "         '../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_minor.xml',\n",
      "         '../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_small.xml',\n",
      "         '../testdata/thredds/acdisc.xml']\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[0])\n",
      "print files[0], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[1])\n",
      "print files[1], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[2])\n",
      "print files[2], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[3])\n",
      "print files[3], filesize, ent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "../testdata/thredds/stellwagen_tsdata_catalog_partial.xml 1182 4.90021731832\n",
        "../testdata/thredds/stellwagen_tsdata_catalog.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7561 5.29295061933\n",
        "../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_minor.xml 1182 4.8983940163\n",
        "../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_small.xml 764 5.00913142639\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.metrics.pairwise import linear_kernel\n",
      "from sklearn import metrics\n",
      "\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "_stopwords = set(stopwords.words('english'))\n",
      "\n",
      "import re\n",
      "\n",
      "def strip_punctuation(text):\n",
      "    simple_pattern = r'[;|>+:=.,<?(){}`\\'\"]'\n",
      "    text = re.sub(simple_pattern, ' ', text)\n",
      "    return text.replace(\"/\", ' ')\n",
      "\n",
      "def tokenize(text):\n",
      "    return word_tokenize(text)\n",
      "    \n",
      "def remove_stopwords(words):\n",
      "    return ' '.join([w for w in words if w not in _stopwords and w])\n",
      "\n",
      "\n",
      "with open(files[0], 'r') as f:\n",
      "    original = f.read()\n",
      "    \n",
      "original = strip_punctuation(original)    \n",
      "original_words = tokenize(original)\n",
      "# print text0_words[0:10]\n",
      "original_words = remove_stopwords(original_words)\n",
      "original = ''.join([t for t in original_words if t])\n",
      "# print original\n",
      "\n",
      "with open(files[1], 'r') as f:\n",
      "    compare = f.read()\n",
      "    \n",
      "compare = strip_punctuation(compare)    \n",
      "compare_words = tokenize(compare)\n",
      "compare_words = remove_stopwords(compare_words)\n",
      "compare = ''.join([t for t in compare_words if t])\n",
      "# print text1\n",
      "\n",
      "tfidf_vectorizer = TfidfVectorizer()\n",
      "tfidf_matrix_trainer = tfidf_vectorizer.fit_transform([original, compare])\n",
      "\n",
      "cos_sim = cosine_similarity(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer)\n",
      "print cos_sim\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.         0.3130777]]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = '''oxygen RNGSchema http www.tei-c.org cms system modules org.tei.www _common schemas teilite.rnc type compact TEI xmlns http www.tei-c.org ns 1.0 xml id di01 n digitallitteratur teiHeader fileDesc titleStmt title Digital Litteratur title titleStmt publicationStmt p For inclusion in the TEI Application Page p publicationStmt sourceDesc bibl Information provided by Donald Broady, e-mail of 29 June 1996 bibl sourceDesc fileDesc profileDesc textClass keywords scheme none term type subject Literary Texts term term type language Swedish including Old Medieval Swedish term keywords textClass profileDesc revisionDesc change when 2007-09-21 date 21 September 2007 date name Chris Ruotolo name Converted to TEI P5 change change date when 2001-12-10 10 December 2001 date label edit label p name Stuart Brown name Included type language att in Swedish keyword minor edit URL checked and OK p change change date when 2000-06-16 16 June 2000 date label update label p name Frances Condron name Updated the contact information p change change date when 1996-08-12 12 August 1996 date label edit label p name WP name Added attribution to text. p change change date when 1996-06-25 25 June 1996 date label create label p name WP name Created file p change revisionDesc teiHeader text body p list type simple item hi rend bold Host hi ILU, Uppsala University item item hi rend bold URL hi ptr target http www.skeptron.uu.se broady dl item list p p hi rend bold Description hi p p Digital Litteratur is a research program at Nada Department of Numerical Analysis and Computing Science , Royal Institute of Technology, Stockholm, in collaboration with the ILU, Uppsala University. The aim is to develop methods and tools for the encoding, management, and distribution of information in accordance with relevant international standards as SGML Standard Generalized Markup Language, ISO 8879 1986 , HyTime Hypermedia Time-based Structuring Language, ISO 10744 1992 , and DSSSL Document Style Semantics and Specification Language, ISO IEC 10179 1996 . A main focus is on applications in the humanities, using TEI s guidelines. Ongoing projects include digital editions of the Swedish author August Strindberg s works, medieval and 16th century records of land conditions, and 17th century court records. p p \\ufffd\\ufffd\\ufffd Donald Broady and Web page p p hi rend bold Funders hi list type simple item FRN Swedish Council for Planning and Co-ordination of Research item item Skolverket Swedish National Agency for Education item item Riksantikvarie E4mbetet Central Board of National Antiquities item list p p hi rend bold Contact hi address addrLine Donald Broady addrLine addrLine Email ref target mailto broady nada.kth.se broady nada.kth.se ref addrLine address p body text TEI'''\n",
      "tokenize(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "['oxygen',\n",
        " 'RNGSchema',\n",
        " 'http',\n",
        " 'www.tei-c.org',\n",
        " 'cms',\n",
        " 'system',\n",
        " 'modules',\n",
        " 'org.tei.www',\n",
        " '_common',\n",
        " 'schemas',\n",
        " 'teilite.rnc',\n",
        " 'type',\n",
        " 'compact',\n",
        " 'TEI',\n",
        " 'xmlns',\n",
        " 'http',\n",
        " 'www.tei-c.org',\n",
        " 'ns',\n",
        " '1.0',\n",
        " 'xml',\n",
        " 'id',\n",
        " 'di01',\n",
        " 'n',\n",
        " 'digitallitteratur',\n",
        " 'teiHeader',\n",
        " 'fileDesc',\n",
        " 'titleStmt',\n",
        " 'title',\n",
        " 'Digital',\n",
        " 'Litteratur',\n",
        " 'title',\n",
        " 'titleStmt',\n",
        " 'publicationStmt',\n",
        " 'p',\n",
        " 'For',\n",
        " 'inclusion',\n",
        " 'in',\n",
        " 'the',\n",
        " 'TEI',\n",
        " 'Application',\n",
        " 'Page',\n",
        " 'p',\n",
        " 'publicationStmt',\n",
        " 'sourceDesc',\n",
        " 'bibl',\n",
        " 'Information',\n",
        " 'provided',\n",
        " 'by',\n",
        " 'Donald',\n",
        " 'Broady',\n",
        " ',',\n",
        " 'e-mail',\n",
        " 'of',\n",
        " '29',\n",
        " 'June',\n",
        " '1996',\n",
        " 'bibl',\n",
        " 'sourceDesc',\n",
        " 'fileDesc',\n",
        " 'profileDesc',\n",
        " 'textClass',\n",
        " 'keywords',\n",
        " 'scheme',\n",
        " 'none',\n",
        " 'term',\n",
        " 'type',\n",
        " 'subject',\n",
        " 'Literary',\n",
        " 'Texts',\n",
        " 'term',\n",
        " 'term',\n",
        " 'type',\n",
        " 'language',\n",
        " 'Swedish',\n",
        " 'including',\n",
        " 'Old',\n",
        " 'Medieval',\n",
        " 'Swedish',\n",
        " 'term',\n",
        " 'keywords',\n",
        " 'textClass',\n",
        " 'profileDesc',\n",
        " 'revisionDesc',\n",
        " 'change',\n",
        " 'when',\n",
        " '2007-09-21',\n",
        " 'date',\n",
        " '21',\n",
        " 'September',\n",
        " '2007',\n",
        " 'date',\n",
        " 'name',\n",
        " 'Chris',\n",
        " 'Ruotolo',\n",
        " 'name',\n",
        " 'Converted',\n",
        " 'to',\n",
        " 'TEI',\n",
        " 'P5',\n",
        " 'change',\n",
        " 'change',\n",
        " 'date',\n",
        " 'when',\n",
        " '2001-12-10',\n",
        " '10',\n",
        " 'December',\n",
        " '2001',\n",
        " 'date',\n",
        " 'label',\n",
        " 'edit',\n",
        " 'label',\n",
        " 'p',\n",
        " 'name',\n",
        " 'Stuart',\n",
        " 'Brown',\n",
        " 'name',\n",
        " 'Included',\n",
        " 'type',\n",
        " 'language',\n",
        " 'att',\n",
        " 'in',\n",
        " 'Swedish',\n",
        " 'keyword',\n",
        " 'minor',\n",
        " 'edit',\n",
        " 'URL',\n",
        " 'checked',\n",
        " 'and',\n",
        " 'OK',\n",
        " 'p',\n",
        " 'change',\n",
        " 'change',\n",
        " 'date',\n",
        " 'when',\n",
        " '2000-06-16',\n",
        " '16',\n",
        " 'June',\n",
        " '2000',\n",
        " 'date',\n",
        " 'label',\n",
        " 'update',\n",
        " 'label',\n",
        " 'p',\n",
        " 'name',\n",
        " 'Frances',\n",
        " 'Condron',\n",
        " 'name',\n",
        " 'Updated',\n",
        " 'the',\n",
        " 'contact',\n",
        " 'information',\n",
        " 'p',\n",
        " 'change',\n",
        " 'change',\n",
        " 'date',\n",
        " 'when',\n",
        " '1996-08-12',\n",
        " '12',\n",
        " 'August',\n",
        " '1996',\n",
        " 'date',\n",
        " 'label',\n",
        " 'edit',\n",
        " 'label',\n",
        " 'p',\n",
        " 'name',\n",
        " 'WP',\n",
        " 'name',\n",
        " 'Added',\n",
        " 'attribution',\n",
        " 'to',\n",
        " 'text',\n",
        " '.',\n",
        " 'p',\n",
        " 'change',\n",
        " 'change',\n",
        " 'date',\n",
        " 'when',\n",
        " '1996-06-25',\n",
        " '25',\n",
        " 'June',\n",
        " '1996',\n",
        " 'date',\n",
        " 'label',\n",
        " 'create',\n",
        " 'label',\n",
        " 'p',\n",
        " 'name',\n",
        " 'WP',\n",
        " 'name',\n",
        " 'Created',\n",
        " 'file',\n",
        " 'p',\n",
        " 'change',\n",
        " 'revisionDesc',\n",
        " 'teiHeader',\n",
        " 'text',\n",
        " 'body',\n",
        " 'p',\n",
        " 'list',\n",
        " 'type',\n",
        " 'simple',\n",
        " 'item',\n",
        " 'hi',\n",
        " 'rend',\n",
        " 'bold',\n",
        " 'Host',\n",
        " 'hi',\n",
        " 'ILU',\n",
        " ',',\n",
        " 'Uppsala',\n",
        " 'University',\n",
        " 'item',\n",
        " 'item',\n",
        " 'hi',\n",
        " 'rend',\n",
        " 'bold',\n",
        " 'URL',\n",
        " 'hi',\n",
        " 'ptr',\n",
        " 'target',\n",
        " 'http',\n",
        " 'www.skeptron.uu.se',\n",
        " 'broady',\n",
        " 'dl',\n",
        " 'item',\n",
        " 'list',\n",
        " 'p',\n",
        " 'p',\n",
        " 'hi',\n",
        " 'rend',\n",
        " 'bold',\n",
        " 'Description',\n",
        " 'hi',\n",
        " 'p',\n",
        " 'p',\n",
        " 'Digital',\n",
        " 'Litteratur',\n",
        " 'is',\n",
        " 'a',\n",
        " 'research',\n",
        " 'program',\n",
        " 'at',\n",
        " 'Nada',\n",
        " 'Department',\n",
        " 'of',\n",
        " 'Numerical',\n",
        " 'Analysis',\n",
        " 'and',\n",
        " 'Computing',\n",
        " 'Science',\n",
        " ',',\n",
        " 'Royal',\n",
        " 'Institute',\n",
        " 'of',\n",
        " 'Technology',\n",
        " ',',\n",
        " 'Stockholm',\n",
        " ',',\n",
        " 'in',\n",
        " 'collaboration',\n",
        " 'with',\n",
        " 'the',\n",
        " 'ILU',\n",
        " ',',\n",
        " 'Uppsala',\n",
        " 'University',\n",
        " '.',\n",
        " 'The',\n",
        " 'aim',\n",
        " 'is',\n",
        " 'to',\n",
        " 'develop',\n",
        " 'methods',\n",
        " 'and',\n",
        " 'tools',\n",
        " 'for',\n",
        " 'the',\n",
        " 'encoding',\n",
        " ',',\n",
        " 'management',\n",
        " ',',\n",
        " 'and',\n",
        " 'distribution',\n",
        " 'of',\n",
        " 'information',\n",
        " 'in',\n",
        " 'accordance',\n",
        " 'with',\n",
        " 'relevant',\n",
        " 'international',\n",
        " 'standards',\n",
        " 'as',\n",
        " 'SGML',\n",
        " 'Standard',\n",
        " 'Generalized',\n",
        " 'Markup',\n",
        " 'Language',\n",
        " ',',\n",
        " 'ISO',\n",
        " '8879',\n",
        " '1986',\n",
        " ',',\n",
        " 'HyTime',\n",
        " 'Hypermedia',\n",
        " 'Time-based',\n",
        " 'Structuring',\n",
        " 'Language',\n",
        " ',',\n",
        " 'ISO',\n",
        " '10744',\n",
        " '1992',\n",
        " ',',\n",
        " 'and',\n",
        " 'DSSSL',\n",
        " 'Document',\n",
        " 'Style',\n",
        " 'Semantics',\n",
        " 'and',\n",
        " 'Specification',\n",
        " 'Language',\n",
        " ',',\n",
        " 'ISO',\n",
        " 'IEC',\n",
        " '10179',\n",
        " '1996',\n",
        " '.',\n",
        " 'A',\n",
        " 'main',\n",
        " 'focus',\n",
        " 'is',\n",
        " 'on',\n",
        " 'applications',\n",
        " 'in',\n",
        " 'the',\n",
        " 'humanities',\n",
        " ',',\n",
        " 'using',\n",
        " 'TEI',\n",
        " 's',\n",
        " 'guidelines',\n",
        " '.',\n",
        " 'Ongoing',\n",
        " 'projects',\n",
        " 'include',\n",
        " 'digital',\n",
        " 'editions',\n",
        " 'of',\n",
        " 'the',\n",
        " 'Swedish',\n",
        " 'author',\n",
        " 'August',\n",
        " 'Strindberg',\n",
        " 's',\n",
        " 'works',\n",
        " ',',\n",
        " 'medieval',\n",
        " 'and',\n",
        " '16th',\n",
        " 'century',\n",
        " 'records',\n",
        " 'of',\n",
        " 'land',\n",
        " 'conditions',\n",
        " ',',\n",
        " 'and',\n",
        " '17th',\n",
        " 'century',\n",
        " 'court',\n",
        " 'records',\n",
        " '.',\n",
        " 'p',\n",
        " 'p',\n",
        " '\\\\ufffd\\\\ufffd\\\\ufffd',\n",
        " 'Donald',\n",
        " 'Broady',\n",
        " 'and',\n",
        " 'Web',\n",
        " 'page',\n",
        " 'p',\n",
        " 'p',\n",
        " 'hi',\n",
        " 'rend',\n",
        " 'bold',\n",
        " 'Funders',\n",
        " 'hi',\n",
        " 'list',\n",
        " 'type',\n",
        " 'simple',\n",
        " 'item',\n",
        " 'FRN',\n",
        " 'Swedish',\n",
        " 'Council',\n",
        " 'for',\n",
        " 'Planning',\n",
        " 'and',\n",
        " 'Co-ordination',\n",
        " 'of',\n",
        " 'Research',\n",
        " 'item',\n",
        " 'item',\n",
        " 'Skolverket',\n",
        " 'Swedish',\n",
        " 'National',\n",
        " 'Agency',\n",
        " 'for',\n",
        " 'Education',\n",
        " 'item',\n",
        " 'item',\n",
        " 'Riksantikvarie',\n",
        " 'E4mbetet',\n",
        " 'Central',\n",
        " 'Board',\n",
        " 'of',\n",
        " 'National',\n",
        " 'Antiquities',\n",
        " 'item',\n",
        " 'list',\n",
        " 'p',\n",
        " 'p',\n",
        " 'hi',\n",
        " 'rend',\n",
        " 'bold',\n",
        " 'Contact',\n",
        " 'hi',\n",
        " 'address',\n",
        " 'addrLine',\n",
        " 'Donald',\n",
        " 'Broady',\n",
        " 'addrLine',\n",
        " 'addrLine',\n",
        " 'Email',\n",
        " 'ref',\n",
        " 'target',\n",
        " 'mailto',\n",
        " 'broady',\n",
        " 'nada.kth.se',\n",
        " 'broady',\n",
        " 'nada.kth.se',\n",
        " 'ref',\n",
        " 'addrLine',\n",
        " 'address',\n",
        " 'p',\n",
        " 'body',\n",
        " 'text',\n",
        " 'TEI']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import glob\n",
      "import os\n",
      "\n",
      "## for the bag of words with xml element names (cheap structure)\n",
      "# this ignores html encoding cruft.\n",
      "\n",
      "def _strip_punctuation(text, simple_pattern=r'[;|>+:=#@%<?(){}`\\'\"]', replace_char=' '):\n",
      "    text = re.sub(simple_pattern, replace_char, text)\n",
      "    return text.replace(\"/\", ' ')\n",
      "def tokenize(text):\n",
      "    return word_tokenize(text)\n",
      "\n",
      "files = glob.glob('../testdata/solr_20150320/clean_20150325_p/*_cleaned.json')\n",
      "outdir = '../testdata/solr_20150320/bow_xml'\n",
      "for f in files:\n",
      "    with open(f, 'r') as g:\n",
      "        data = json.loads(g.read())\n",
      "\n",
      "    content = data['content']\n",
      "    digest = data['digest']\n",
      "    content = _strip_punctuation(content)\n",
      "    words = tokenize(content)\n",
      "    bag = remove_stopwords(words)\n",
      "    bag = _strip_punctuation(bag, r'[.,]', '')\n",
      "        \n",
      "    with open(os.path.join(outdir, '%s.txt' % digest), 'w') as g:\n",
      "        g.write(bag)\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## for the bag of words with any text element/attribute value but no tags\n",
      "from lxml import etree \n",
      "\n",
      "# taken from the Parser obj\n",
      "parser = etree.XMLParser(\n",
      "            encoding='utf-8',\n",
      "            ns_clean=True,\n",
      "            remove_blank_text=True,\n",
      "            remove_comments=True,\n",
      "            recover=True\n",
      "        )\n",
      "\n",
      "# modified to just return any text\n",
      "def find_nodes(xml):\n",
      "    nodes = []\n",
      "    for elem in xml.iter():\n",
      "        t = elem.text.strip() if elem.text else ''\n",
      "        tags = [elem.tag] + [e.tag for e in elem.iterancestors()]\n",
      "        tags.reverse()\n",
      "\n",
      "        # check for an assumed comment object (shouldn't be here at all)\n",
      "        if sum([isinstance(a, str) for a in tags]) != len(tags):\n",
      "            continue\n",
      "\n",
      "        att_texts = parse_node_attributes(elem)\n",
      "        nodes += [a for a in [t] + att_texts if a]\n",
      "    return nodes\n",
      "\n",
      "def parse_node_attributes(node):\n",
      "    if not node.attrib:\n",
      "        return []\n",
      "    return node.attrib.values() if node.attrib else []\n",
      "\n",
      "files = glob.glob('../testdata/solr_20150320/clean_20150325_p/*_cleaned.json')\n",
      "outdir = '../testdata/solr_20150320/bow_no_xml'\n",
      "for f in files:\n",
      "    with open(f, 'r') as g:\n",
      "        data = json.loads(g.read())\n",
      "\n",
      "    content = data['content'].encode('unicode_escape')\n",
      "    digest = data['digest']\n",
      "    \n",
      "    xml = etree.fromstring(content, parser=parser)\n",
      "    \n",
      "    content = ' '.join(find_nodes(xml))\n",
      "    content = _strip_punctuation(content)\n",
      "    words = tokenize(content)\n",
      "    bag = remove_stopwords(words)\n",
      "    bag = _strip_punctuation(bag, r'[.,]', '')\n",
      "    \n",
      "    if bag:\n",
      "        with open(os.path.join(outdir, '%s.txt' % digest), 'w') as g:\n",
      "            try:\n",
      "                g.write(bag)\n",
      "            except UnicodeEncodeError:\n",
      "                g.write(bag.encode('ascii', 'ignore'))\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'NoneType' object has no attribute 'iter'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-14-0e52beaa5171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mxml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strip_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-14-0e52beaa5171>\u001b[0m in \u001b[0;36mfind_nodes\u001b[0;34m(xml)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterancestors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'iter'"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}