{
 "metadata": {
  "name": "",
  "signature": "sha256:6d4dbaa42ade6f64b589bdf93e2f5628aa3891c13ab9472abd9cfecf7984f160"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Notes on detecting text similarity between unparsed response text\n",
      "\n",
      "1. byte histogram/shannon's entropy?\n",
      "2. tf-idf (but issues with xml strings, etc)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# file_entropy.py\n",
      "#\n",
      "# Shannon Entropy of a file\n",
      "# = minimum average number of bits per character\n",
      "# required for encoding (compressing) the file\n",
      "#\n",
      "# So the theoretical limit (in bytes) for data compression:\n",
      "# Shannon Entropy of the file * file size (in bytes) / 8\n",
      "# (Assuming the file is a string of byte-size (UTF-8?) characters\n",
      "# because if not then the Shannon Entropy value would be different.)\n",
      "# FB - 201011291\n",
      "import math\n",
      "import os\n",
      "\n",
      "def calculate_entropy(filename):\n",
      "    # read the whole file into a byte array\n",
      "    with open(filename, \"rb\") as f:\n",
      "        byteArr = map(ord, f.read())\n",
      "\n",
      "    fileSize = len(byteArr)\n",
      "\n",
      "    # calculate the frequency of each byte value in the file\n",
      "    freqList = []\n",
      "    for b in range(256):\n",
      "        ctr = 0\n",
      "        for byte in byteArr:\n",
      "            if byte == b:\n",
      "                ctr += 1\n",
      "        freqList.append(float(ctr) / fileSize)\n",
      "\n",
      "    # Shannon entropy\n",
      "    ent = 0.0\n",
      "    for freq in freqList:\n",
      "        if freq > 0:\n",
      "            ent = ent + freq * math.log(freq, 2)\n",
      "    ent = -ent\n",
      "#     print 'Shannon entropy (min bits per byte-character):'\n",
      "#     print ent\n",
      "    return fileSize, freqList, ent\n",
      "\n",
      "files = ['../testdata/thredds/stellwagen_tsdata_catalog_partial.xml', \n",
      "         '../testdata/thredds/stellwagen_tsdata_catalog.xml',\n",
      "         '../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_minor.xml',\n",
      "         '../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_small.xml',\n",
      "         '../testdata/thredds/acdisc.xml']\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[0])\n",
      "print files[0], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[1])\n",
      "print files[1], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[2])\n",
      "print files[2], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[3])\n",
      "print files[3], filesize, ent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "../testdata/thredds/stellwagen_tsdata_catalog_partial.xml 1182 4.90021731832\n",
        "../testdata/thredds/stellwagen_tsdata_catalog.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7561 5.29295061933\n",
        "../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_minor.xml 1182 4.8983940163\n",
        "../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_small.xml 764 5.00913142639\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.metrics.pairwise import linear_kernel\n",
      "from sklearn import metrics\n",
      "\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "_stopwords = set(stopwords.words('english'))\n",
      "\n",
      "import re\n",
      "\n",
      "def strip_punctuation(text):\n",
      "    simple_pattern = r'[;|>+:=.,<?(){}`\\'\"]'\n",
      "    text = re.sub(simple_pattern, ' ', text)\n",
      "    return text.replace(\"/\", ' ')\n",
      "\n",
      "def tokenize(text):\n",
      "    return word_tokenize(text)\n",
      "    \n",
      "def remove_stopwords(words):\n",
      "    return ' '.join([w for w in words if w not in _stopwords and w])\n",
      "\n",
      "\n",
      "with open(files[0], 'r') as f:\n",
      "    original = f.read()\n",
      "    \n",
      "original = strip_punctuation(original)    \n",
      "original_words = tokenize(original)\n",
      "# print text0_words[0:10]\n",
      "original_words = remove_stopwords(original_words)\n",
      "original = ''.join([t for t in original_words if t])\n",
      "# print original\n",
      "\n",
      "with open(files[1], 'r') as f:\n",
      "    compare = f.read()\n",
      "    \n",
      "compare = strip_punctuation(compare)    \n",
      "compare_words = tokenize(compare)\n",
      "compare_words = remove_stopwords(compare_words)\n",
      "compare = ''.join([t for t in compare_words if t])\n",
      "# print text1\n",
      "\n",
      "tfidf_vectorizer = TfidfVectorizer()\n",
      "tfidf_matrix_trainer = tfidf_vectorizer.fit_transform([original, compare])\n",
      "\n",
      "cos_sim = cosine_similarity(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer)\n",
      "print cos_sim\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.         0.3130777]]\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}