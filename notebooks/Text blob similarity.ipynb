{
 "metadata": {
  "name": "",
  "signature": "sha256:9a9ebcf019170f80c886150acb4263ca00cf0fff016a22eff84e435496a74e48"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Notes on detecting text similarity between unparsed response text\n",
      "\n",
      "1. byte histogram/shannon's entropy?\n",
      "2. tf-idf (but issues with xml strings, etc)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# file_entropy.py\n",
      "#\n",
      "# Shannon Entropy of a file\n",
      "# = minimum average number of bits per character\n",
      "# required for encoding (compressing) the file\n",
      "#\n",
      "# So the theoretical limit (in bytes) for data compression:\n",
      "# Shannon Entropy of the file * file size (in bytes) / 8\n",
      "# (Assuming the file is a string of byte-size (UTF-8?) characters\n",
      "# because if not then the Shannon Entropy value would be different.)\n",
      "# FB - 201011291\n",
      "import math\n",
      "import os\n",
      "\n",
      "def calculate_entropy(filename):\n",
      "    # read the whole file into a byte array\n",
      "    with open(filename, \"rb\") as f:\n",
      "        byteArr = map(ord, f.read())\n",
      "\n",
      "    fileSize = len(byteArr)\n",
      "\n",
      "    # calculate the frequency of each byte value in the file\n",
      "    freqList = []\n",
      "    for b in range(256):\n",
      "        ctr = 0\n",
      "        for byte in byteArr:\n",
      "            if byte == b:\n",
      "                ctr += 1\n",
      "        freqList.append(float(ctr) / fileSize)\n",
      "\n",
      "    # Shannon entropy\n",
      "    ent = 0.0\n",
      "    for freq in freqList:\n",
      "        if freq > 0:\n",
      "            ent = ent + freq * math.log(freq, 2)\n",
      "    ent = -ent\n",
      "#     print 'Shannon entropy (min bits per byte-character):'\n",
      "#     print ent\n",
      "    return fileSize, freqList, ent\n",
      "\n",
      "files = ['../testdata/thredds/stellwagen_tsdata_catalog_partial.xml', \n",
      "         '../testdata/thredds/stellwagen_tsdata_catalog.xml',\n",
      "         '../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_minor.xml',\n",
      "         '../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_small.xml',\n",
      "         '../testdata/thredds/acdisc.xml']\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[0])\n",
      "print files[0], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[1])\n",
      "print files[1], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[2])\n",
      "print files[2], filesize, ent\n",
      "\n",
      "filesize, frequencies, ent = calculate_entropy(files[3])\n",
      "print files[3], filesize, ent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "../testdata/thredds/stellwagen_tsdata_catalog_partial.xml 1182 4.90021731832\n",
        "../testdata/thredds/stellwagen_tsdata_catalog.xml"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7561 5.29295061933\n",
        "../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_minor.xml 1182 4.8983940163\n",
        "../testdata/thredds/for_similarity_checks/stellwagen_tsdata_catalog_partial_small.xml 764 5.00913142639\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.metrics.pairwise import linear_kernel\n",
      "from sklearn import metrics\n",
      "\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "_stopwords = set(stopwords.words('english'))\n",
      "\n",
      "import re\n",
      "\n",
      "def strip_punctuation(text):\n",
      "    simple_pattern = r'[;|>+:=.,<?(){}`\\'\"]'\n",
      "    text = re.sub(simple_pattern, ' ', text)\n",
      "    return text.replace(\"/\", ' ')\n",
      "\n",
      "def tokenize(text):\n",
      "    return word_tokenize(text)\n",
      "    \n",
      "def remove_stopwords(words):\n",
      "    return ' '.join([w for w in words if w not in _stopwords and w])\n",
      "\n",
      "\n",
      "with open(files[0], 'r') as f:\n",
      "    original = f.read()\n",
      "    \n",
      "original = strip_punctuation(original)    \n",
      "original_words = tokenize(original)\n",
      "# print text0_words[0:10]\n",
      "original_words = remove_stopwords(original_words)\n",
      "original = ''.join([t for t in original_words if t])\n",
      "# print original\n",
      "\n",
      "with open(files[1], 'r') as f:\n",
      "    compare = f.read()\n",
      "    \n",
      "compare = strip_punctuation(compare)    \n",
      "compare_words = tokenize(compare)\n",
      "compare_words = remove_stopwords(compare_words)\n",
      "compare = ''.join([t for t in compare_words if t])\n",
      "# print text1\n",
      "\n",
      "tfidf_vectorizer = TfidfVectorizer()\n",
      "tfidf_matrix_trainer = tfidf_vectorizer.fit_transform([original, compare])\n",
      "\n",
      "cos_sim = cosine_similarity(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer)\n",
      "print cos_sim\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.         0.3130777]]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import glob\n",
      "import os\n",
      "\n",
      "## for the bag of words with xml element names (cheap structure)\n",
      "# this ignores html encoding cruft.\n",
      "\n",
      "def _strip_punctuation(text, simple_pattern=r'[;|>+:=#@%<?(){}`\\'\"]', replace_char=' '):\n",
      "    text = re.sub(simple_pattern, replace_char, text)\n",
      "    return text.replace(\"/\", ' ')\n",
      "def tokenize(text):\n",
      "    return word_tokenize(text)\n",
      "\n",
      "files = glob.glob('../testdata/solr_20150320/clean_20150325_p/*_cleaned.json')\n",
      "outdir = '../testdata/solr_20150320/bow_xml'\n",
      "for f in files:\n",
      "    with open(f, 'r') as g:\n",
      "        data = json.loads(g.read())\n",
      "\n",
      "    content = data['content']\n",
      "    digest = data['digest']\n",
      "    content = _strip_punctuation(content)\n",
      "    words = tokenize(content)\n",
      "    bag = remove_stopwords(words)\n",
      "    bag = _strip_punctuation(bag, r'[.,]', '')\n",
      "        \n",
      "    with open(os.path.join(outdir, '%s.txt' % digest), 'w') as g:\n",
      "        g.write(bag)\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## for the bag of words with any text element/attribute value but no tags\n",
      "from lxml import etree \n",
      "\n",
      "# taken from the Parser obj\n",
      "parser = etree.XMLParser(\n",
      "            encoding='utf-8',\n",
      "            ns_clean=True,\n",
      "            remove_blank_text=True,\n",
      "            remove_comments=True,\n",
      "            recover=True\n",
      "        )\n",
      "\n",
      "# modified to just return any text\n",
      "def find_nodes(xml):\n",
      "    nodes = []\n",
      "    for elem in xml.iter():\n",
      "        t = elem.text.strip() if elem.text else ''\n",
      "        tags = [elem.tag] + [e.tag for e in elem.iterancestors()]\n",
      "        tags.reverse()\n",
      "\n",
      "        # check for an assumed comment object (shouldn't be here at all)\n",
      "        if sum([isinstance(a, str) for a in tags]) != len(tags):\n",
      "            continue\n",
      "\n",
      "        att_texts = parse_node_attributes(elem)\n",
      "        nodes += [a for a in [t] + att_texts if a]\n",
      "    return nodes\n",
      "\n",
      "def parse_node_attributes(node):\n",
      "    if not node.attrib:\n",
      "        return []\n",
      "    return node.attrib.values() if node.attrib else []\n",
      "\n",
      "files = glob.glob('../testdata/solr_20150320/clean_20150325_p/*_cleaned.json')\n",
      "outdir = '../testdata/solr_20150320/bow_no_xml'\n",
      "for f in files:\n",
      "    with open(f, 'r') as g:\n",
      "        data = json.loads(g.read())\n",
      "\n",
      "    content = data['content'].encode('unicode_escape')\n",
      "    digest = data['digest']\n",
      "    \n",
      "    xml = etree.fromstring(content, parser=parser)\n",
      "    \n",
      "    if xml is None:\n",
      "        print 'NO XML: ', digest\n",
      "        continue\n",
      "    \n",
      "    nodes = find_nodes(xml)\n",
      "    content = ' '.join(nodes if nodes else [])\n",
      "    if not content:\n",
      "        print digest\n",
      "        continue\n",
      "    content = _strip_punctuation(content)\n",
      "    words = tokenize(content)\n",
      "    bag = remove_stopwords(words)\n",
      "    bag = _strip_punctuation(bag, r'[.,]', '')\n",
      "    \n",
      "    if bag:\n",
      "        with open(os.path.join(outdir, '%s.txt' % digest), 'w') as g:\n",
      "            try:\n",
      "                g.write(bag)\n",
      "            except UnicodeEncodeError:\n",
      "                g.write(bag.encode('ascii', 'ignore'))\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "04b219d820811a6cb1bc7af03b52b626\n",
        "0f161c6e18155f31a0c522d6434693ba"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12fc3bfc1b4f39a14032e9da8e592d11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "166d257ebc0e1be17d96e79c383c1b4b"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "184715fdf06bd9fc835dc62929e8d36f"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "27d44e4d5e96d3db5a8cf834b44b69a5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4c93c9e87b669cf74473b08be29483e2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5135ab69f05c6744087f2b2af19e1d15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5a77078bdfdb9408fb9155d7a9466f8f"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "61826bdd9307ff8d905712d3fbce7d29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6918ba9dc992d2c524730da594bd6dab"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ae8dc130b36b17e47127b1e69acba194"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NO XML: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " b5a4080d703e06e78402d86588c61291\n",
        "bf19615c45ddba4cbba611db639fe499"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "d888644c0238d7e21709bb7e80acccfd"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NO XML: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " db208a9b9ac4feed80a870ffc3490ef1\n",
        "f6a1bcedaa977efeb1b0b706c0659877"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "f7582357e363bbe21087b828272d5a47"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Looking for similar documents (without xml)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.metrics.pairwise import linear_kernel\n",
      "from sklearn import metrics\n",
      "import glob\n",
      "import os\n",
      "\n",
      "# for each file, run against every other.\n",
      "# so let's load them all into a 2d matrix, digest, bag of words\n",
      "bags = []\n",
      "files = glob.glob('../testdata/solr_20150320/bow_no_xml/*.txt')\n",
      "for f in files:\n",
      "    if '_similarities' in f:\n",
      "        continue\n",
      "    i = f.split('/')[-1].replace('.txt', '')\n",
      "    with open(f, 'r') as g:\n",
      "        bow = g.read()\n",
      "    \n",
      "    bags.append([i, bow])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# similarity scoring\n",
      "import time\n",
      "\n",
      "start_time = time.time()\n",
      "\n",
      "ptimes = []\n",
      "\n",
      "# do the tokenization of all the bags first \n",
      "# this is not a quick little thing\n",
      "all_identifiers = [b[0] for b in bags]\n",
      "all_bags = [b[1] for b in bags]\n",
      "\n",
      "tfidf_vectorizer = TfidfVectorizer()\n",
      "\n",
      "for i, bag in enumerate(bags):\n",
      "    if os.path.exists('../testdata/solr_20150320/bow_no_xml/%s_similarities.txt' % bag[0]):\n",
      "        continue\n",
      "    tf_start = time.time()\n",
      "    tf_identifiers = [all_identifiers[i]] + all_identifiers[:i-1] + all_identifiers[i:]\n",
      "    tf_data = [all_bags[i]] + all_bags[:i-1] + all_bags[i:]\n",
      "    \n",
      "    tfidf_matrix_trainer = tfidf_vectorizer.fit_transform(tf_data)\n",
      "    \n",
      "    cos_sim = cosine_similarity(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer)\n",
      "    \n",
      "    lk = linear_kernel(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer).flatten()\n",
      "\n",
      "    related_indices = cos_sim.argsort()[:-len(tf_data)-1:-1] \n",
      "    related_indices_with_lk_value = [r for r in related_indices[0] if lk[r] >= 0.1]\n",
      "    related_indices_with_lk_value.reverse()\n",
      "\n",
      "    related_set = [(lk[k], tf_data[k], tf_identifiers[k]) for k in related_indices_with_lk_value]\n",
      "    lines = []\n",
      "    for cs, b, d in related_set[1:]:\n",
      "        #print cs, b, d\n",
      "        lines.append('%s,%s' % (cs, d))\n",
      "    \n",
      "    ptimes.append(time.time() - tf_start)\n",
      "    if i % 50 == 0:\n",
      "        print time.time() - tf_start\n",
      "    \n",
      "    with open('../testdata/solr_20150320/bow_no_xml/%s_similarities.txt' % bag[0], 'w') as g:\n",
      "        g.write('\\n'.join(lines))\n",
      "\n",
      "print time.time() - start_time\n",
      "\n",
      "print 'average time:', sum(ptimes) / len(ptimes)\n",
      "print 'longest running:', max(ptimes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "47.7222130299\n",
        "48.5523808002"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "48.7056849003"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49.0535950661"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "52.9939260483"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "51.7386889458"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-d23cc49a6b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_bags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mall_bags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mall_bags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtfidf_matrix_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcos_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix_trainer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_matrix_trainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \"\"\"\n\u001b[0;32m-> 1282\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                     \u001b[0;31m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "one: at least one response near 1.0 sim that has two entries in solr (two md5s) for one url (the tiger line rdf - the diff is that one element has a closing tag (`<tag thing=\"\"/>` vs `<tag thing=\"\"></tag>`)\n",
      "\n",
      "<img src=\"screenshots/rdf_compare_tigerlines.png\"/>\n",
      "\n",
      "hahaha. it's https vs http\n",
      "\n",
      "two: ddx files from some data collection and close in time/space have high similarity values\n",
      "\n",
      "three: what to do about binary base 64 image blobs\n",
      "\n",
      "four: what to do about numeric data (ddx, kml)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Calculating Jaccard's Similarity"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# by hand\n",
      "set_a = set(['a', 'b', 'c', 'd'])\n",
      "set_b = set(['c', 'd', 'e', 'f', 'g'])\n",
      "\n",
      "n = len(set_a.intersection(set_b))\n",
      "print n / float(len(set_a) + len(set_b) - n)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sklearn\n",
      "import numpy as np\n",
      "from sklearn.metrics import jaccard_similarity_score\n",
      "jaccard_similarity_score([0, 2, 1, 3], [0, 1, 2, 3])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "0.5"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are not the same things. label sets with true and predicted and same shape vs set intersections.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from here: http://sujitpal.blogspot.com/2013/05/inter-document-similarity-with-scikit.html\n",
      "\n",
      "import numpy as np\n",
      "import scipy.sparse as ss\n",
      "from sklearn.feature_extraction.text import FeatureHasher\n",
      "\n",
      "def _s_pos_or_zero(x):\n",
      "  return x if x > 0 else 0\n",
      "\n",
      "def _s_zero_mask(x, y):\n",
      "  return 0 if y == 0 else x\n",
      "\n",
      "def _s_safe_divide(x, y):\n",
      "  return 0 if x == 0 or y == 0 else x / y\n",
      "\n",
      "_v_pos_or_zero = np.vectorize(_s_pos_or_zero)\n",
      "_v_zero_mask = np.vectorize(_s_zero_mask)\n",
      "_v_safe_divide = np.vectorize(_s_safe_divide)\n",
      "\n",
      "def _assymetric_subset_measure(doc1, doc2):\n",
      "  epsilon = np.ones(doc1.shape) * 2\n",
      "  filtered = _v_pos_or_zero(epsilon - (_v_safe_divide(doc1, doc2) +\n",
      "    _v_safe_divide(doc2, doc1)))\n",
      "  zdoc1 = _v_zero_mask(doc1, filtered)\n",
      "  zdoc2 = _v_zero_mask(doc2, filtered)\n",
      "  return np.sum(np.dot(zdoc1, zdoc2)) / np.sum(np.dot(doc1, doc2))\n",
      "\n",
      "def scam_distance(doc1, doc2):\n",
      "    # scam distance from: http://www.iaeng.org/publication/IMECS2011/IMECS2011_pp272-277.pdf\n",
      "  asm12 = _assymetric_subset_measure(doc1, doc2)\n",
      "  asm21 = _assymetric_subset_measure(doc2, doc1)\n",
      "  return max(asm12, asm21)\n",
      "\n",
      "# let's take a couple of bags of words, to vectors, and see what happens\n",
      "# and the bags are from a set with a cosine-similarity score already\n",
      "# source response 00a6a7c286cd5e079fe986da5b7c61bb (one of the acm catalog.data.gov files)\n",
      "\n",
      "'''\n",
      "our comparison responses (cosine similarity scores)\n",
      "\n",
      "0.847771238354,9ecc22095444da85bbb62f1ff378a810\n",
      "0.70041372101,4ea98f5dcd26223afc5d7e046c0c4f07\n",
      "0.300917729507,12ffab6a6e20d943104e9a435ed20564\n",
      "0.143943462532,9143ef758c3b01374b3af15b49313a81\n",
      "'''\n",
      "\n",
      "source_bag = '''https catalogdatagov dataset 2005-2009-american-community-survey-5-year-estimates-summary-file-tracts-and-blo urn uuid DOC-4310 A nationwide survey collects information age  race  income  commute time work  home value  veteran status  data  Data American Community Survey Puerto Rico Community Survey collected calendar years 2005- 2009  Data available block group level  acs demographic population age sex disability education employment income ancestry language poverty race ethnicity hispanic latino origin relationships veteran status grandparents marital status marriage divorce fertility school enrollment housing tenure renters owners homeowner physical characteristics financial characteristics number rooms mortgage household size family type economic industry grandparents caregivers journey work occupation health insurance food stamps snap place birth social veterans citizenship employment disability https catalogdatagov dataset 2005-2009-american-community-survey-5-year-estimates-summary-file-tracts-and-blo 2005-2009-american-community-survey-5-year-estimates-summary-file-tracts-and-blo 2005-2009-american-community-survey-5-year-estimates-summary-file-tracts-and-blo 2005-2009 American Community Survey 5-Year Estimates Summary File Tracts Block Groups http www2censusgov acs2009_5yr summaryfile 2005-2009_ACSSF_All_In_2_Giant_Files 28Experienced-Users-Only 29 Tracts_Block_Groups_Onlyzip csv txt  xls csv txt  xls Web Page data-quality TRUE __category_tag_3b4e71c5-3c3c-43af-a3ff-81694225e453 [ Housing Community ] contact-email sharonmstern censusgov data-dictiionary http wwwcensusgov acs www data_documentation documentation_main issued 12 14 2010 dataset-reference-date January 1  2005-December 31  2009 resource-type Dataset __category_tag_b6db7c5a-d306-4da5-a0cd-b4128e1322ce [ Total Energy ] theme Section 1  Population __category_tag_9a350fa9-bc49-43d4-8e77-270b9714976d [ Environment  Biodiversity  Coral  Asian  Native Hawaiian  Pacific Islander ] references http wwwcensusgov acs www methodology methodology_main  wwwcensusgov acs www data_documentation documentation_main frequency-of-update Annual metadata-date 12 14 2010 access-level public spatial-text United States Puerto Rico granularity Block group metadata-source dms'''\n",
      "bags = {\n",
      "    \"9ecc22095444da85bbb62f1ff378a810\": \"https catalogdatagov dataset 2005-2009-american-community-survey-5-year-estimates-summary-file urn uuid DOC-10941 A nationwide survey collects information age  race  income  commute time work  home value  veteran status  data  Data American Community Survey Puerto Rico Community Survey collected calendar years 2005- 2009  Data available small geographies  Census tract block group data available another dataset  acs demographic population age sex education employment income ancestry language poverty race ethnicity hispanic latino origin relationships veteran status grandparents marital status marriage divorce fertility school enrollment housing tenure renters owners homeowner physical characteristics financial characteristics number rooms mortgage household size family type economic industry grandparents caregivers journey work occupation health insurance food stamps snap place birth social veterans citizenship employment  https catalogdatagov dataset 2005-2009-american-community-survey-5-year-estimates-summary-file 2005-2009-american-community-survey-5-year-estimates-summary-file 2005-2009-american-community-survey-5-year-estimates-summary-file 2005-2009 American Community Survey 5-Year Estimates Summary File http www2censusgov acs2009_5yr summaryfile 2005-2009_ACSSF_All_In_2_Giant_Files 28Experienced-Users-Only 29 All_Geographies_Not_Tracts_Block_Groupszip csv txt  xls csv txt  xls Web Page data-quality TRUE contact-email tasharboone censusgov data-dictiionary http wwwcensusgov acs www data_documentation documentation_main issued 12 14 2010 dataset-reference-date January 1  2005-December 31  2009 resource-type Dataset theme Section 1  Population __category_tag_9a350fa9-bc49-43d4-8e77-270b9714976d [ Environment  Biodiversity  Coral  Asian  Native Hawaiian  Pacific Islander ] references http wwwcensusgov acs www methodology methodology_main  wwwcensusgov acs www data_documentation documentation_main frequency-of-update Annual metadata-date 12 14 2010 access-level public spatial-text United States Puerto Rico granularity Places county subdivisions metadata-source dms\",\n",
      "    \"4ea98f5dcd26223afc5d7e046c0c4f07\": \"https catalogdatagov dataset 2006-2010-american-community-survey-5-year-estimates-summary-file urn uuid DOC-5661 A nationwide survey collects information age  race  income  commute time work  home value  veteran status  data  Data American Community Survey Puerto Rico Community Survey collected calendar years 2006-2010  Data available small geographies  Census tract block group data available another dataset  acs demographic population age sex education employment income ancestry language poverty race ethnicity hispanic latino origin relationships veteran status grandparents marital status marriage divorce fertility school enrollment housing tenure renters owners homeowner physical characteristics financial characteristics number rooms mortgage household size family type economic industry grandparents caregivers journey work occupation health insurance food stamps snap place birth social veterans citizenship employment  https catalogdatagov dataset 2006-2010-american-community-survey-5-year-estimates-summary-file 2006-2010-american-community-survey-5-year-estimates-summary-file 2006-2010-american-community-survey-5-year-estimates-summary-file 2006-2010 American Community Survey 5-Year Estimates Summary File http www2censusgov acs2010_5yr summaryfile 2006-2010_ACSSF_All_In_2_Giant_Files 28Experienced-Users-Only 29 All_Geographies_Not_Tracts_Block_Groupszip TXT TXT Text File data-quality TRUE contact-email sharonmstern censusgov data-dictiionary http wwwcensusgov acs www data_documentation documentation_main issued 12-08-2011 dataset-reference-date Jan 1  2006 - Dec 31  2010 resource-type Dataset __category_tag_b6db7c5a-d306-4da5-a0cd-b4128e1322ce [ Electricity ] theme Section 1  Population __category_tag_9a350fa9-bc49-43d4-8e77-270b9714976d [ Environment  Hawaii  Guam  American Samoa  Asian  Native Hawaiian  Pacific Islander ] references http www2censusgov acs2010_5yr summaryfile UserTools ACS_2006-2010_SF_Tech_Docpdf frequency-of-update Annual metadata-date 12-08-2011 access-level public spatial-text United States Puerto Rico granularity Places county subdivisions metadata-source dms\",\n",
      "    \"12ffab6a6e20d943104e9a435ed20564\": \"http catalogdatagov dataset american-factfinder-ii urn uuid DOC-5156 American FactFinder Census Bureau online  self-service tool designed search variety population  economic  geographic housing information  demographic population age sex disability education employment income ancestry language poverty race ethnicity hispanic origin relationships veteran status grandparents marital status fertility school enrollment housing tenure renters owners physical characteristics financial characteristics number rooms household size family type economic business industry mining utilities construction manufacturing wholesale trade retail trade transportation warehousing information finance insurance real estate rental leasing professional scientific technical services management companies enterprises administrative support waste management remediation services educational services health care social assistance art entertainment recreation accommodation food services services establishments sales shipment receipts number employees payroll http catalogdatagov dataset american-factfinder-ii american-factfinder-ii american-factfinder-ii American FactFinder II http factfinder2censusgov CSV CSV Comma Separated Values File data-quality TRUE __category_tag_3b4e71c5-3c3c-43af-a3ff-81694225e453 [ Housing Community ] contact-email jeffreydsisson censusgov data-dictiionary http factfinder2censusgov help en american_factfinder_helphtm glossary glossaryhtm issued 01 18 2011 dataset-reference-date 2000 - Present resource-type Dataset theme Section 1  Population __category_tag_9a350fa9-bc49-43d4-8e77-270b9714976d [ Immigration  Country Birth  Education  Asian  Pacific Islander  Languages  Limited English Proficiency LEP  Housing  Employment Labor  Income  Veterans  Native Hawaiian ] references http factfinder2censusgov faces help jsf pages metadataxhtml lang en frequency-of-update Decennial - Every 10 Years  Quinquennial - Every 5 years  Annual licence metadata-date Ongoing weekly releases access-level public spatial-text United States  Puerto Rico  US Virgin Islands  Commonwealth Northern Mariana Islands  Guam  American Samoa granularity Multiple based specific data set interest including national  regional  state  county  local block level  Statistical political geographic hierarchies available  metadata-source dms\",\n",
      "    \"9143ef758c3b01374b3af15b49313a81\": \"https catalogdatagov dataset fips-county-code-look-up-tool urn uuid DOC-5459 The US Census Bureau online County Look-up Tool provides unique 3-digit code Identification Counties Equivalent Entities United States  Possessions  Insular Areas  ansi fips county codes state codes fips 6-4 incits 31 2009 https catalogdatagov dataset fips-county-code-look-up-tool fips-county-code-look-up-tool fips-county-code-look-up-tool FIPS County Code Look-up Tool http wwwcensusgov geo www ansi countylookuphtml TXT TXT Text File data-quality TRUE contact-email geogeography censusgov data-dictiionary http wwwcensusgov geo www ansi ansihtml issued 01 01 2000 resource-type Dataset dataset-reference-date 2010 spatial-text United States  Puerto Rico  US Possessions theme Section 6  Geography Environment references http wwwcensusgov geo www ansi ansihtml frequency-of-update As needed metadata-date 02 04 2010 access-level public granularity County metadata-source dms\"\n",
      "}\n",
      "\n",
      "# let us now try the feature hashing as reasonable option\n",
      "\n",
      "hasher = FeatureHasher(input_type=\"string\")\n",
      "source_matrix = hasher.transform(source_bag.split(' '))\n",
      "#print source_matrix\n",
      "\n",
      "for identifier, bag in bags.iteritems():\n",
      "    compare_matrix = hasher.transform(bag.split(' '))\n",
      "    scam = scam_distance(source_matrix, compare_matrix)\n",
      "    \n",
      "    print identifier, ' scam distance: ', scam\n",
      "\n",
      "# humbug and no more. the shapes won't match but that scam code is not quite enough to know.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 8294)\t-1.0\n",
        "  (0, 341263)\t-1.0\n",
        "  (0, 532659)\t-2.0\n",
        "  (0, 871725)\t-1.0\n",
        "  (1, 13227)\t-1.0\n",
        "  (1, 296051)\t2.0\n",
        "  (1, 334826)\t-2.0\n",
        "  (1, 354738)\t4.0\n",
        "  (1, 532659)\t-2.0\n",
        "  (1, 595059)\t1.0\n",
        "  (1, 862625)\t-1.0\n",
        "  (1, 879148)\t1.0\n",
        "  (2, 341263)\t-1.0\n",
        "  (2, 354738)\t2.0\n",
        "  (2, 532659)\t-2.0\n",
        "  (2, 595059)\t1.0\n",
        "  (2, 803687)\t1.0\n",
        "  (3, 13227)\t-1.0\n",
        "  (3, 15072)\t3.0\n",
        "  (3, 98813)\t-1.0\n",
        "  (3, 226289)\t5.0\n",
        "  (3, 276572)\t6.0\n",
        "  (3, 296051)\t2.0\n",
        "  (3, 297019)\t1.0\n",
        "  (3, 341263)\t-5.0\n",
        "  :\t:\n",
        "  (231, 927572)\t-1.0\n",
        "  (232, 154159)\t-1.0\n",
        "  (232, 296051)\t1.0\n",
        "  (232, 628086)\t-1.0\n",
        "  (232, 862625)\t-1.0\n",
        "  (232, 879148)\t1.0\n",
        "  (233, 8294)\t-1.0\n",
        "  (233, 15072)\t1.0\n",
        "  (233, 226289)\t1.0\n",
        "  (233, 296051)\t1.0\n",
        "  (233, 334826)\t-1.0\n",
        "  (234, 15072)\t1.0\n",
        "  (234, 226289)\t1.0\n",
        "  (234, 276572)\t1.0\n",
        "  (234, 296051)\t1.0\n",
        "  (234, 341263)\t-1.0\n",
        "  (234, 354738)\t3.0\n",
        "  (234, 532659)\t-2.0\n",
        "  (234, 595059)\t1.0\n",
        "  (234, 699617)\t-1.0\n",
        "  (234, 803687)\t2.0\n",
        "  (234, 862625)\t-1.0\n",
        "  (235, 276572)\t1.0\n",
        "  (235, 341263)\t-1.0\n",
        "  (235, 595059)\t1.0\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-15-96fdc806771f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mcompare_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhasher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mscam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscam_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' scam distance: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-15-96fdc806771f>\u001b[0m in \u001b[0;36mscam_distance\u001b[0;34m(doc1, doc2)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscam_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# scam distance from: http://www.iaeng.org/publication/IMECS2011/IMECS2011_pp272-277.pdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0masm12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_assymetric_subset_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0masm21\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_assymetric_subset_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masm12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masm21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-15-96fdc806771f>\u001b[0m in \u001b[0;36m_assymetric_subset_measure\u001b[0;34m(doc1, doc2)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assymetric_subset_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   filtered = _v_pos_or_zero(epsilon - (_v_safe_divide(doc1, doc2) +\n\u001b[0m\u001b[1;32m     23\u001b[0m     _v_safe_divide(doc2, doc1)))\n\u001b[1;32m     24\u001b[0m   \u001b[0mzdoc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_v_zero_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   1761\u001b[0m             \u001b[0m_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m             \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0;31m# Convert args to object arrays first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36m_get_ufunc_and_otypes\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   1723\u001b[0m             \u001b[0;31m# arrays (the input values are not checked to ensure this)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_a\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m             \u001b[0;31m# Performance note: profiling indicates that -- for simple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-15-96fdc806771f>\u001b[0m in \u001b[0;36m_s_safe_divide\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_s_safe_divide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0m_v_pos_or_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_s_pos_or_zero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m//anaconda/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             raise ValueError(\"The truth value of an array with more than one \"\n\u001b[0m\u001b[1;32m    184\u001b[0m                              \"element is ambiguous. Use a.any() or a.all().\")\n\u001b[1;32m    185\u001b[0m     \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()."
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using pattern just for giggles\n",
      "\n",
      "(i have no idea why and lang id is not a method but a init value?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pattern.vector import Document\n",
      "\n",
      "source_bag = '''https catalogdatagov dataset 2005-2009-american-community-survey-5-year-estimates-summary-file-tracts-and-blo urn uuid DOC-4310 A nationwide survey collects information age  race  income  commute time work  home value  veteran status  data  Data American Community Survey Puerto Rico Community Survey collected calendar years 2005- 2009  Data available block group level  acs demographic population age sex disability education employment income ancestry language poverty race ethnicity hispanic latino origin relationships veteran status grandparents marital status marriage divorce fertility school enrollment housing tenure renters owners homeowner physical characteristics financial characteristics number rooms mortgage household size family type economic industry grandparents caregivers journey work occupation health insurance food stamps snap place birth social veterans citizenship employment disability https catalogdatagov dataset 2005-2009-american-community-survey-5-year-estimates-summary-file-tracts-and-blo 2005-2009-american-community-survey-5-year-estimates-summary-file-tracts-and-blo 2005-2009-american-community-survey-5-year-estimates-summary-file-tracts-and-blo 2005-2009 American Community Survey 5-Year Estimates Summary File Tracts Block Groups http www2censusgov acs2009_5yr summaryfile 2005-2009_ACSSF_All_In_2_Giant_Files 28Experienced-Users-Only 29 Tracts_Block_Groups_Onlyzip csv txt  xls csv txt  xls Web Page data-quality TRUE __category_tag_3b4e71c5-3c3c-43af-a3ff-81694225e453 [ Housing Community ] contact-email sharonmstern censusgov data-dictiionary http wwwcensusgov acs www data_documentation documentation_main issued 12 14 2010 dataset-reference-date January 1  2005-December 31  2009 resource-type Dataset __category_tag_b6db7c5a-d306-4da5-a0cd-b4128e1322ce [ Total Energy ] theme Section 1  Population __category_tag_9a350fa9-bc49-43d4-8e77-270b9714976d [ Environment  Biodiversity  Coral  Asian  Native Hawaiian  Pacific Islander ] references http wwwcensusgov acs www methodology methodology_main  wwwcensusgov acs www data_documentation documentation_main frequency-of-update Annual metadata-date 12 14 2010 access-level public spatial-text United States Puerto Rico granularity Block group metadata-source dms'''\n",
      "\n",
      "d = Document(source_bag, threshold=1)\n",
      "\n",
      "print d.features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'disability', u'rico', u'puerto', u'2010', u'csv', u'race', u'wwwcensusgov', u'xls', u'www', u'employment', u'grandparents', u'acs', u'american', u'survey', u'block', u'community', u'dataset', u'txt', u'housing', u'catalogdatagov', u'https', u'population', u'12', u'14', u'age', u'characteristics', u'veteran', u'1', u'income', u'status', u'http', u'data', u'2009']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}