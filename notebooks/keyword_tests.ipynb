{
 "metadata": {
  "name": "",
  "signature": "sha256:5eeab0b631c16e81b18c98ff1e5a0a4a3f78193b84f148382e4ad3f6c0af8e76"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Keyword Splitting tests\n",
      "\n",
      "See semantics-preprocessing [Issue #8](https://github.com/b-cube/semantics-preprocessing/issues/8)\n",
      "\n",
      "So how do we take some text structure from an unknown, but parsable, metadata source and make it usable by some NLP or other feature characterization method? In some standards, we have a keyword structure that is an implied list (keywords.keyword, etc) but, even within that structure, the free text as \"keyword\" can be some char-delimited list of terms based on the source's metadata generation practices. For example, a keyword text element can be the \">\"-delimited hierarchy of GCMD terms (we can debate the necessity of splitting those structures based on other characterization needs later). \n",
      "\n",
      "In addition, those terms can be phrases so a naive \"split on whitespace\" is an issue.\n",
      "\n",
      "Our test strings as a set of tuples (label, string to test):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "keyword_strings = [\n",
      "    (\"Basic comma-delimited singletons\", \"soil, clay, pedon, loam\"),\n",
      "    (\"Basic comma-delimited singletons and phrases\", \"soil, clay, silty clay, silty loam\"),\n",
      "    (\"Whitespace-delimited singletons (DC, OpenSearch)\", \"soil clay pedon loam\"),\n",
      "    (\"GCMD Term List\", \"EARTH SCIENCE > Oceans > Bathymetry/Seafloor Topography > Seafloor Topography\"),\n",
      "    (\"An actual phrase/sentence\", \"Hydrographic Surveys for Selected Locations Within the United States (hydro_bathy_2006)\"),\n",
      "    (\"Pipe-delimited singletons\", \"soil|clay|pedon|loam\"),\n",
      "    (\"Plus-delimited singletons\", \"soil+clay+pedon+loam+sandy loam\"),\n",
      "    (\"Semicolon-delimited singletons\", \"soil;clay;pedon;loam\")\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The two test strategies - a little regex and a little nltk. The list of potential delimiters: \",\", \";\", \">\", \"|\", \" \", \"+\", \"-\" (this is incomplete, todo: revise for a broad enough set of delims).\n",
      "\n",
      "Let's start with a basic pattern against that set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "simple_pattern = r'[;,|>+]*'\n",
      "\n",
      "for label, keywords in keyword_strings:\n",
      "    terms = re.split(simple_pattern, keywords)\n",
      "    \n",
      "    print label, ' => ', keywords\n",
      "    print '\\t', terms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Basic comma-delimited singletons  =>  soil, clay, pedon, loam\n",
        "\t['soil', ' clay', ' pedon', ' loam']\n",
        "Basic comma-delimited singletons and phrases  =>  soil, clay, silty clay, silty loam\n",
        "\t['soil', ' clay', ' silty clay', ' silty loam']\n",
        "Whitespace-delimited singletons (DC, OpenSearch)  =>  soil clay pedon loam\n",
        "\t['soil clay pedon loam']\n",
        "GCMD Term List  =>  EARTH SCIENCE > Oceans > Bathymetry/Seafloor Topography > Seafloor Topography\n",
        "\t['EARTH SCIENCE ', ' Oceans ', ' Bathymetry/Seafloor Topography ', ' Seafloor Topography']\n",
        "An actual phrase/sentence  =>  Hydrographic Surveys for Selected Locations Within the United States (hydro_bathy_2006)\n",
        "\t['Hydrographic Surveys for Selected Locations Within the United States (hydro_bathy_2006)']\n",
        "Pipe-delimited singletons  =>  soil|clay|pedon|loam\n",
        "\t['soil', 'clay', 'pedon', 'loam']\n",
        "Plus-delimited singletons  =>  soil+clay+pedon+loam+sandy loam\n",
        "\t['soil', 'clay', 'pedon', 'loam', 'sandy loam']\n",
        "Semicolon-delimited singletons  =>  soil;clay;pedon;loam\n",
        "\t['soil', 'clay', 'pedon', 'loam']\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far, so good. We aren't handling whitespace well and there are some \"internal\" terms that could be handled (Bathymetry/Seafloor Topography where we have two concepts, Bathymetry Topography and Seafloor Topography. This is a **scoping problem** to be dealt with later). \n",
      "\n",
      "Let's at least handle the trailing whitespaces (bugs me)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for label, keywords in keyword_strings:\n",
      "    terms = [k.strip() for k in re.split(simple_pattern, keywords)]\n",
      "    \n",
      "    print label, ' => ', keywords\n",
      "    print '\\t', terms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Basic comma-delimited singletons  =>  soil, clay, pedon, loam\n",
        "\t['soil', 'clay', 'pedon', 'loam']\n",
        "Basic comma-delimited singletons and phrases  =>  soil, clay, silty clay, silty loam\n",
        "\t['soil', 'clay', 'silty clay', 'silty loam']\n",
        "Whitespace-delimited singletons (DC, OpenSearch)  =>  soil clay pedon loam\n",
        "\t['soil clay pedon loam']\n",
        "GCMD Term List  =>  EARTH SCIENCE > Oceans > Bathymetry/Seafloor Topography > Seafloor Topography\n",
        "\t['EARTH SCIENCE', 'Oceans', 'Bathymetry/Seafloor Topography', 'Seafloor Topography']\n",
        "An actual phrase/sentence  =>  Hydrographic Surveys for Selected Locations Within the United States (hydro_bathy_2006)\n",
        "\t['Hydrographic Surveys for Selected Locations Within the United States (hydro_bathy_2006)']\n",
        "Pipe-delimited singletons  =>  soil|clay|pedon|loam\n",
        "\t['soil', 'clay', 'pedon', 'loam']\n",
        "Plus-delimited singletons  =>  soil+clay+pedon+loam+sandy loam\n",
        "\t['soil', 'clay', 'pedon', 'loam', 'sandy loam']\n",
        "Semicolon-delimited singletons  =>  soil;clay;pedon;loam\n",
        "\t['soil', 'clay', 'pedon', 'loam']\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On to some noun chunking and light tokenizing. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob import TextBlob\n",
      "\n",
      "# let's just try on one that we know has a phrase\n",
      "# from the second tuple in keyword_strings\n",
      "txt = TextBlob(\"soil, clay, silty clay, silty loam\")\n",
      "\n",
      "txt.noun_phrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "WordList([u'silty clay', u'silty loam'])"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Okay, that was a little unexpected but we can then try to just append the singletons."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "txt.noun_phrases + txt.words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[u'silty clay',\n",
        " u'silty loam',\n",
        " 'soil',\n",
        " 'clay',\n",
        " 'silty',\n",
        " 'clay',\n",
        " 'silty',\n",
        " 'loam']"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now we have duplicates. Let's try this - extract phrases, remove identified phrases, extract words. (Alternately, use a set at the end.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "set(txt.noun_phrases + txt.words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "{'clay', 'loam', 'silty', u'silty clay', u'silty loam', 'soil'}"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SET does something we may not want - capturing the words within the phrases. (Note: honestly do not know today if that is important in our larger keyword comparison work, R.)\n",
      "\n",
      "Back to the original thinking."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re \n",
      "substitutions = re.compile('|'.join(txt.noun_phrases))\n",
      "revised_txt = substitutions.sub('', \"soil, clay, silty clay, silty loam\")\n",
      "\n",
      "revised_txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "'soil, clay, , '"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "revised_blob = TextBlob(revised_txt)\n",
      "\n",
      "txt.noun_phrases + revised_blob.words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[u'silty clay', u'silty loam', 'soil', 'clay']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That got a little ugly there at the end (poor variable management, really). But we now have the list of terms.\n",
      "\n",
      "Up next, what happens across the set of tuples and what do we do about things more closely identified as sentences (our Hydrographic Surveys blob)?\n",
      "\n",
      "A more complete test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "from textblob import TextBlob\n",
      "\n",
      "keyword_strings = [\n",
      "    (\"Basic comma-delimited singletons\", \"soil, clay, pedon, loam\"),\n",
      "    (\"Basic comma-delimited singletons and phrases\", \"soil, clay, silty clay, silty loam\"),\n",
      "    (\"Whitespace-delimited singletons (DC, OpenSearch)\", \"soil clay pedon loam\"),\n",
      "    (\"GCMD Term List\", \"EARTH SCIENCE > Oceans > Bathymetry/Seafloor Topography > Seafloor Topography\"),\n",
      "    (\"An actual phrase/sentence\", \"Hydrographic Surveys for Selected Locations Within the United States (hydro_bathy_2006)\"),\n",
      "    (\"Pipe-delimited singletons\", \"soil|clay|pedon|loam\"),\n",
      "    (\"Plus-delimited singletons\", \"soil+clay+pedon+loam+sandy loam\"),\n",
      "    (\"Semicolon-delimited singletons\", \"soil;clay;pedon;loam\")\n",
      "]\n",
      "\n",
      "for label, terms_as_string in keyword_strings:\n",
      "    # set up the basic text parser\n",
      "    tb = TextBlob(terms_as_string)\n",
      "    phrases = tb.noun_phrases\n",
      "    \n",
      "    # remove the found phrases\n",
      "    substitutions = re.compile('|'.join(phrases))\n",
      "    substituted_text = substitutions.sub('', terms_as_string.lower())\n",
      "    \n",
      "    # set up the second parser\n",
      "    stb = TextBlob(substituted_text)\n",
      "    \n",
      "    # combine the two lists\n",
      "    parsed_terms = phrases + stb.words\n",
      "    \n",
      "    print label\n",
      "    print '\\t', parsed_terms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Basic comma-delimited singletons\n",
        "\t['soil', 'clay', 'pedon', 'loam']\n",
        "Basic comma-delimited singletons and phrases\n",
        "\t[u'silty clay', u'silty loam', 'soil', 'clay']\n",
        "Whitespace-delimited singletons (DC, OpenSearch)\n",
        "\t[u'soil clay pedon loam']\n",
        "GCMD Term List\n",
        "\t[u'earth science', 'oceans', u'bathymetry/seafloor topography', u'seafloor topography']\n",
        "An actual phrase/sentence\n",
        "\t['hydrographic', u'selected locations', 'surveys', 'for', 'within', 'the', 'united', 'states', 'hydro_bathy_2006']\n",
        "Pipe-delimited singletons\n",
        "\t['soil|clay|pedon|loam']\n",
        "Plus-delimited singletons\n",
        "\t[u'soil+clay+pedon+loam+sandy loam', 'soil+clay+pedon+loam+sandy', 'loam']\n",
        "Semicolon-delimited singletons\n",
        "\t['soil', 'clay', 'pedon', 'loam']\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Huh, that does not handle spaces only or pipes/pluses as delimiters. So switch to tokenizers?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import WhitespaceTokenizer\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "\n",
      "whitespace_tokenizer = WhitespaceTokenizer()\n",
      "blob = TextBlob('soil clay pedon loam', tokenizer=whitespace_tokenizer)\n",
      "blob.tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "WordList(['soil', 'clay', 'pedon', 'loam'])"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fair enough - we're going to need some heuristics for catching these.\n",
      "\n",
      "On to the regex tokenizer for pipes. (I like how 'greater than' is trainable in the np extractors, but pipes aren't. It's like how awkward can we make ssurgo data.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "regex_tokenizer = RegexpTokenizer(r'\\w+|(\\w+)*')\n",
      "blob = TextBlob('soil|clay|pedon|loam', tokenizer=regex_tokenizer)\n",
      "print 'tokens alone includes the whitespace: ', blob.tokens\n",
      "[b for b in blob.tokens if b]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tokens alone includes the whitespace:  ['soil', '', 'clay', '', 'pedon', '', 'loam', '']\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "['soil', 'clay', 'pedon', 'loam']"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And so this should get us to words and phrases that are pipe-delimited. Except we could have some sort of pipe-delimited array that contains some comma-delimited strings for phrases (why would you do that?!!) and we all know people that have likely done that. \n",
      "\n",
      "So pipe-delimited to {some normal delimiter}-delimited string to then run against the noun_phrases? Eep. Theoretically, we could just replace the pipes with commas and then chunk it - we don't actually care about retaining the nested structure of that list. It's just nouns and phrases."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}