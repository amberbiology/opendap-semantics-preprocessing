{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cleanup for some bag of words issues (unicode cruft)\n",
    "- extract the namespaces (should be a better way than this, but *waves*)\n",
    "\n",
    "\n",
    "oh. well. we simply can't handle the emoji: http://www.unicode.org/L2/L2015/15049-C-emoji-annotations.xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from lxml import etree\n",
    "\n",
    "#files = glob.glob('../solr_superset/docs/*.json')\n",
    "\n",
    "# let's strip out all of the namespaces and make a stopwords list for the identifiers\n",
    "\n",
    "files = glob.glob('../solr_superset/identified/*.json')\n",
    "\n",
    "master_namespaces = set()\n",
    "\n",
    "for f in files:\n",
    "    with open(f, 'r') as g:\n",
    "        data = json.loads(g.read())\n",
    "    \n",
    "    content = data['content'].encode('unicode_escape')\n",
    "    \n",
    "    try:\n",
    "        xml = etree.fromstring(content)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    document_namespaces = dict(xml.xpath('/*/namespace::*'))\n",
    "    if None in document_namespaces:\n",
    "        document_namespaces['default'] = document_namespaces[None]\n",
    "        del document_namespaces[None]\n",
    "\n",
    "    # now run through any child namespace issues\n",
    "    all_namespaces = xml.xpath('//namespace::*')\n",
    "    for i, ns in enumerate(all_namespaces):\n",
    "        if ns[1] in document_namespaces.values():\n",
    "            continue\n",
    "        new_key = ns[0] if ns[0] else 'default%s' % i\n",
    "        document_namespaces[new_key] = ns[1]\n",
    "\n",
    "    # and get the values\n",
    "    found_namespaces = set(document_namespaces.values())\n",
    "        \n",
    "    master_namespaces = master_namespaces.symmetric_difference(found_namespaces)\n",
    "\n",
    "#master_namespaces\n",
    "with open('../solr_superset/namespaces.txt', 'w') as f:\n",
    "    f.write('\\n'.join(list(master_namespaces)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"s</annotation> dd <annotation cp='[ - - - - {# }{0 }{1 }{2 }{3 }{4 }{5 }{6 }{7 }{8 }{9 }]'>word</annotation> <annotation cp='[ ]'>worker</annotation> <annotation cp='[ - ]'>world</annotation> <annotation cp='[ ]'>worried</annotation> <annotation cp='[ ]'>wrench</annotation> <annotation cp='[ ]'>write</annotation> <annotation cp='[ ]'>yellow</annotation> <annotation cp='[{ }]'>yemen</annotation> <annotation cp='[ ]'>yen</annotation> <annotation cp='[{ }]'>zambia</annotation> <annotation cp='[{ }]'>zimbabwe</annotation> <annotation cp='[ - ]'>zodiac</annotation> <annotation cp='[ ]'>zzz</annotation> </annotations> </ldml>\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pttn = ur'[\\\\{2,}ufffd]'\n",
    "\n",
    "some_text = '''\\\\\\\\ufffd\\\\ufffd\\\\\\\\ufffds</annotation> dd  <annotation cp='[\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd-\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd-\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd-\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd-\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd{#\\\\ufffd\\\\ufffd\\\\ufffd}{0\\\\ufffd\\\\ufffd\\\\ufffd}{1\\\\ufffd\\\\ufffd\\\\ufffd}{2\\\\ufffd\\\\ufffd\\\\ufffd}{3\\\\ufffd\\\\ufffd\\\\ufffd}{4\\\\ufffd\\\\ufffd\\\\ufffd}{5\\\\ufffd\\\\ufffd\\\\ufffd}{6\\\\ufffd\\\\ufffd\\\\ufffd}{7\\\\ufffd\\\\ufffd\\\\ufffd}{8\\\\ufffd\\\\ufffd\\\\ufffd}{9\\\\ufffd\\\\ufffd\\\\ufffd}]'>word</annotation>   <annotation cp='[\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd]'>worker</annotation>   <annotation cp='[\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd-\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd]'>world</annotation>   <annotation cp='[\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd]'>worried</annotation>   <annotation cp='[\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd]'>wrench</annotation>   <annotation cp='[\\\\ufffd\\\\ufffd\\\\ufffd]'>write</annotation>   <annotation cp='[\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd]'>yellow</annotation>   <annotation cp='[{\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd}]'>yemen</annotation>   <annotation cp='[\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd]'>yen</annotation>   <annotation cp='[{\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd}]'>zambia</annotation>   <annotation cp='[{\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd}]'>zimbabwe</annotation>   <annotation cp='[\\\\ufffd\\\\ufffd\\\\ufffd-\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd]'>zodiac</annotation>   <annotation cp='[\\\\ufffd\\\\ufffd\\\\ufffd\\\\ufffd]'>zzz</annotation>  </annotations> </ldml>'''\n",
    "\n",
    "#new_text = re.sub(pttn, ' ', some_text)\n",
    "new_text = some_text.replace('\\\\\\\\ufffd', ' ').replace('\\\\ufffd', ' ')\n",
    "\n",
    "' '.join(new_text.split())\n",
    "\n",
    "#some_text.replace(u'\\\\\\\\ufffd', ' ').replace(u'\\\\ufffd', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the volume distribution $ rm( langle V rangle langle V sp3 rangle ...).$ As long as the ensemble'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_text = '''the volume distribution $\\\\rm(\\\\langle V\\\\rangle,\\\\langle V\\\\sp3\\\\rangle,\\\\...).$ As long as the ensemble'''\n",
    "\n",
    "pttn = ur'[\\\\{2,}]'\n",
    "\n",
    "new_text = re.sub(pttn, ' ', some_text)\n",
    "\n",
    "' '.join(new_text.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here's some  stuff  hi  but   no\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'here\\'s some, stuff [hi] but : no'\n",
    "\n",
    "simple_pattern = r'[;|>+:=.,()/?!\\[\\]]'\n",
    "\n",
    "re.sub(simple_pattern, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some ***   are things. - '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's play with numbers\n",
    "\n",
    "some_text = '''some cat's 435634 are things. -34252'''\n",
    "\n",
    "pttn = ur'\\w*\\b-?\\d\\s*\\w*'\n",
    "\n",
    "captures = re.findall(pttn, some_text)\n",
    "\n",
    "stuff = re.sub('|'.join(captures), ' ', some_text)\n",
    "\n",
    "pttn = \"cat's\"\n",
    "\n",
    "re.sub(pttn, '***', stuff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../lib/corpus/mimetypes.txt', 'r') as f:\n",
    "    mimes = [a.strip() for a in f.readlines()]\n",
    "    \n",
    "for m in mimes:\n",
    "    try:\n",
    "        x = re.compile(m)\n",
    "    except Exception as ex:\n",
    "        print m\n",
    "        print '\\t', ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's play with the bag of words steps because that is just annoying\n",
    "# also bad tests. meep.\n",
    "\n",
    "import re\n",
    "import collections\n",
    "import json\n",
    "import HTMLParser\n",
    "import nltk\n",
    "import yaml\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import WordListCorpusReader\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "_corpus_root = '../lib/corpus'\n",
    "\n",
    "\n",
    "def flatten(items, excluded_keys=[]):\n",
    "    def _flatten(item):\n",
    "        if isinstance(item, dict):\n",
    "            for k, v in item.iteritems():\n",
    "                if k in excluded_keys:\n",
    "                    continue\n",
    "                # TODO: this introduces nested lists again!\n",
    "                yield list(_flatten(v))\n",
    "        elif isinstance(item, list):\n",
    "            for i in item:\n",
    "                if isinstance(i, collections.Iterable) and not isinstance(i, basestring):\n",
    "                    for subitem in _flatten(i):\n",
    "                        yield subitem\n",
    "                else:\n",
    "                    yield i\n",
    "        elif isinstance(item, str):\n",
    "            yield item\n",
    "\n",
    "    arr = list(_flatten(items))\n",
    "    if len([isinstance(a, collections.Iterable) and\n",
    "            not isinstance(a, basestring) for a in arr]) > 0:\n",
    "        # ick, flatten the dict issue again.\n",
    "        return list(_flatten(arr))\n",
    "\n",
    "    return arr\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    simple_pattern = r'[;|>+:=.,()/?!\\[\\]{}]'\n",
    "    return re.sub(simple_pattern, ' ', text)\n",
    "\n",
    "\n",
    "def split_words(text):\n",
    "    ''' '''\n",
    "    simple_pattern = r'[/:.]'\n",
    "    return re.split(simple_pattern, text)\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    _stopwords = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    words = words if isinstance(words, list) else words.split()\n",
    "    return ' '.join([w for w in words if w not in _stopwords and w])\n",
    "\n",
    "\n",
    "def remove_mimetypes(text):\n",
    "    mimetypes = WordListCorpusReader(_corpus_root, 'mimetypes.txt')\n",
    "    words = [w.replace('+', '\\+') for w in mimetypes.words()]\n",
    "\n",
    "    pttn = re.compile('|'.join(words))\n",
    "    return pttn.sub(' ', text)\n",
    "\n",
    "\n",
    "def remove_numeric(text):\n",
    "    match_pttn = ur'\\w*\\b-?\\d\\s*\\w*'\n",
    "    captures = re.findall(match_pttn, text)\n",
    "\n",
    "    # strip them out\n",
    "    if captures:\n",
    "        return re.sub(re.compile('|'.join(captures)), ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_mimetypes(text, do_replace=True):\n",
    "    mimetypes = WordListCorpusReader(_corpus_root, 'mimetypes.txt')\n",
    "\n",
    "    found_mimetypes = [w for w in mimetypes.words() if w in text]\n",
    "\n",
    "    if do_replace:\n",
    "        text = remove_mimetypes(text)\n",
    "\n",
    "    return found_mimetypes, text\n",
    "\n",
    "\n",
    "def collapse_to_bag(data_blob, exclude_urls=True):\n",
    "    _url_keys = ['url']\n",
    "    excludes = _url_keys if exclude_urls else []\n",
    "\n",
    "    # TODO: run the generator\n",
    "    bag_of_words = flatten(data_blob, excludes)\n",
    "    bag_of_words = bag_of_words if isinstance(bag_of_words, list) else bag_of_words.split()\n",
    "\n",
    "    return ' '.join(bag_of_words)\n",
    "\n",
    "\n",
    "def normalize_subjects(subjects, do_split=False, return_unique=True):\n",
    "    if not subjects:\n",
    "        return\n",
    "\n",
    "    normalized_subjects = []\n",
    "    for subject in subjects:\n",
    "        if not subject:\n",
    "            # skip any empty tag\n",
    "            continue\n",
    "        normalized = normalize_keyword_text(subject)\n",
    "        normalized_subjects += [n.strip() for n in normalized.split(',') if n] \\\n",
    "            if do_split else [normalized]\n",
    "\n",
    "    return list(set(normalized_subjects)) if return_unique else normalized_subjects\n",
    "\n",
    "\n",
    "def normalize_keyword_text(keyword_string):\n",
    "    if not keyword_string:\n",
    "        return\n",
    "\n",
    "    # unescape\n",
    "    hp = HTMLParser.HTMLParser()\n",
    "    keyword_string = hp.unescape(keyword_string)\n",
    "\n",
    "    # replace underscores (assume these are NOT delimiters\n",
    "    #    but phrase concatenators)\n",
    "    underscore_pattern = r'[_]'\n",
    "    keyword_string = re.sub(underscore_pattern, ' ', keyword_string)\n",
    "\n",
    "    punctuation_pattern = r'[;|>+:=+(){}]'\n",
    "    return re.sub(punctuation_pattern, ',', keyword_string)\n",
    "\n",
    "def _normalize_keywords(service_description):\n",
    "    service = service_description.get('service', {})\n",
    "    if not service:\n",
    "        return service_description\n",
    "    subjects = service.get('subject', [])\n",
    "    if not subjects:\n",
    "        return service_description\n",
    "\n",
    "    # return split and as a unique list\n",
    "    service['subject'] = normalize_subjects(subjects, True, True)\n",
    "    service_description['service'] = service\n",
    "    return service_description\n",
    "\n",
    "def parse_yaml(yaml_file):\n",
    "    with open(yaml_file, 'r') as f:\n",
    "        y = yaml.load(f.read())\n",
    "    return y\n",
    "\n",
    "def extract_task_config(yaml_config, task):\n",
    "    return yaml_config.get(task, {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'normalize_keywords': True, 'remove_mimetypes': True, 'remove_stopwords': True, 'remove_numeric': True, 'remove_punctuation': True}\n",
      "\n",
      "normalize_keywords\n",
      "\tPacific Science Center inspires a lifelong interest in science, math and technology by engaging diverse communities through interactive and innovative exhibits and programs. Pacific Science Center\n",
      "\n",
      "remove_mimetypes\n",
      "\tPacific Science Center inspires a lifelong interest in science, math and technology by engaging diverse communities through interactive and innovative exhibits and programs. Pacific Science Center\n",
      "\n",
      "remove_stopwords\n",
      "\tPacific Science Center inspires lifelong interest science , math technology engaging diverse communities interactive innovative exhibits programs . Pacific Science Center\n",
      "\n",
      "remove_numeric\n",
      "\tPacific Science Center inspires lifelong interest science , math technology engaging diverse communities interactive innovative exhibits programs . Pacific Science Center\n",
      "\n",
      "remove_punctuation\n",
      "\tPacific Science Center inspires lifelong interest science   math technology engaging diverse communities interactive innovative exhibits programs   Pacific Science Center\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and the actual running of the thing\n",
    "\n",
    "config = parse_yaml('../tasks/bagofwords_from_parsed_20150504.yaml')\n",
    "config = extract_task_config(config, 'BagOfWordsFromParsed')\n",
    "tasks = config.get('tasks', {})\n",
    "\n",
    "print tasks\n",
    "print \n",
    "\n",
    "with open('../solr_superset/parsed/006bc0af7c797ea4636812a8e3154178_parsed.json', 'r') as f:\n",
    "    data = json.loads(f.read())\n",
    "    \n",
    "service_description = data.get('service_description', {})\n",
    "\n",
    "service_description = _normalize_keywords(service_description)\n",
    "\n",
    "#service_description['service']['subject']\n",
    "    \n",
    "# it's not here    \n",
    "bow = collapse_to_bag(data)\n",
    "\n",
    "for k, v in tasks.iteritems():\n",
    "    if k == 'remove_mimetypes':\n",
    "        bow = remove_mimetypes(bow)\n",
    "    elif k == 'remove_punctuation':\n",
    "        bow = remove_punctuation(bow)\n",
    "    elif k == 'remove_stopwords':\n",
    "        bow = remove_stopwords(bow)\n",
    "    elif k == 'remove_numeric':\n",
    "        bow = remove_numeric(bow)\n",
    "    \n",
    "    print k\n",
    "    print '\\t', bow\n",
    "    print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "x = '''Pacific Science Center inspires a lifelong interest in science, math and technology by \n",
    "engaging diverse communities through interactive and innovative exhibits and programs. Pacific Science Center'''\n",
    "\n",
    "match_pttn = ur'\\w*\\b-?\\d\\s*\\w*'\n",
    "captures = re.findall(match_pttn, text)\n",
    "\n",
    "print captures\n",
    "\n",
    "if captures:\n",
    "    re.sub(re.compile('|'.join(captures)), ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
